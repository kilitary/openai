{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# How to work with large language models\n",
    "\n",
    "## How large language models work\n",
    "\n",
    "[Large language models][Large language models Blog Post] are functions that map text to text. Given an input string of text, a large language model predicts the text that should come next.\n",
    "\n",
    "The magic of large language models is that by being trained to minimize this prediction error over vast quantities of text, the models end up learning concepts useful for these predictions. For example, they learn:\n",
    "\n",
    "* how to spell\n",
    "* how grammar works\n",
    "* how to paraphrase\n",
    "* how to answer questions\n",
    "* how to hold a conversation\n",
    "* how to write in many languages\n",
    "* how to code\n",
    "* etc.\n",
    "\n",
    "None of these capabilities are explicitly programmed in—they all emerge as a result of training.\n",
    "\n",
    "GPT-3 powers [hundreds of software products][GPT3 Apps Blog Post], including productivity apps, education apps, games, and more.\n",
    "\n",
    "## How to control a large language model\n",
    "\n",
    "Of all the inputs to a large language model, by far the most influential is the text prompt.\n",
    "\n",
    "Large language models can be prompted to produce output in a few ways:\n",
    "\n",
    "* **Instruction**: Tell the model what you want\n",
    "* **Completion**: Induce the model to complete the beginning of what you want\n",
    "* **Demonstration**: Show the model what you want, with either:\n",
    "  * A few examples in the prompt\n",
    "  * Many hundreds or thousands of examples in a fine-tuning training dataset\n",
    "\n",
    "An example of each is shown below.\n",
    "\n",
    "### Instruction prompts\n",
    "\n",
    "Instruction-following models (e.g., `text-davinci-003` or any model beginning with `text-`) are specially designed to follow instructions. Write your instruction at the top of the prompt (or at the bottom, or both), and the model will do its best to follow the instruction and then stop. Instructions can be detailed, so don't be afraid to write a paragraph explicitly detailing the output you want.\n",
    "\n",
    "Example instruction prompt:\n",
    "\n",
    "```text\n",
    "Extract the name of the author from the quotation below.\n",
    "\n",
    "“Some humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.”\n",
    "― Ted Chiang, Exhalation\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```text\n",
    "Ted Chiang\n",
    "```\n",
    "\n",
    "### Completion prompt example\n",
    "\n",
    "Completion-style prompts take advantage of how large language models try to write text they think is mostly likely to come next. To steer the model, try beginning a pattern or sentence that will be completed by the output you want to see. Relative to direct instructions, this mode of steering large language models can take more care and experimentation. In addition, the models won't necessarily know where to stop, so you will often need stop sequences or post-processing to cut off text generated beyond the desired output.\n",
    "\n",
    "Example completion prompt:\n",
    "\n",
    "```text\n",
    "“Some humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.”\n",
    "― Ted Chiang, Exhalation\n",
    "\n",
    "The author of this quote is\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```text\n",
    " Ted Chiang\n",
    "```\n",
    "\n",
    "### Demonstration prompt example (few-shot learning)\n",
    "\n",
    "Similar to completion-style prompts, demonstrations can show the model what you want it to do. This approach is sometimes called few-shot learning, as the model learns from a few examples provided in the prompt.\n",
    "\n",
    "Example demonstration prompt:\n",
    "\n",
    "```text\n",
    "Quote:\n",
    "“When the reasoning mind is forced to confront the impossible again and again, it has no choice but to adapt.”\n",
    "― N.K. Jemisin, The Fifth Season\n",
    "Author: N.K. Jemisin\n",
    "\n",
    "Quote:\n",
    "“Some humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.”\n",
    "― Ted Chiang, Exhalation\n",
    "Author:\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```text\n",
    " Ted Chiang\n",
    "```\n",
    "\n",
    "### Fine-tuned prompt example\n",
    "\n",
    "With enough training examples, you can [fine-tune][Fine Tuning Docs] a custom model. In this case, instructions become unnecessary, as the model can learn the task from the training data provided. However, it can be helpful to include separator sequences (e.g., `->` or `###` or any string that doesn't commonly appear in your inputs) to tell the model when the prompt has ended and the output should begin. Without separator sequences, there is a risk that the model continues elaborating on the input text rather than starting on the answer you want to see.\n",
    "\n",
    "Example fine-tuned prompt (for a model that has been custom trained on similar prompt-completion pairs):\n",
    "\n",
    "```text\n",
    "“Some humans theorize that intelligent species go extinct before they can expand into outer space. If they're correct, then the hush of the night sky is the silence of the graveyard.”\n",
    "― Ted Chiang, Exhalation\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```text\n",
    " Ted Chiang\n",
    "```\n",
    "\n",
    "## Code Capabilities\n",
    "\n",
    "Large language models aren't only great at text - they can be great at code too. OpenAI's specialized code model is called [Codex].\n",
    "\n",
    "Codex powers [more than 70 products][Codex Apps Blog Post], including:\n",
    "\n",
    "* [GitHub Copilot] (autocompletes code in VS Code and other IDEs)\n",
    "* [Pygma](https://pygma.app/) (turns Figma designs into code)\n",
    "* [Replit](https://replit.com/) (has an 'Explain code' button and other features)\n",
    "* [Warp](https://www.warp.dev/) (a smart terminal with AI command search)\n",
    "* [Machinet](https://machinet.net/) (writes Java unit test templates)\n",
    "\n",
    "Note that unlike instruction-following text models (e.g., `text-davinci-002`), Codex is *not* trained to follow instructions. As a result, designing good prompts can take more care.\n",
    "\n",
    "### More prompt advice\n",
    "\n",
    "For more prompt examples, visit [OpenAI Examples][OpenAI Examples].\n",
    "\n",
    "In general, the input prompt is the best lever for improving model outputs. You can try tricks like:\n",
    "\n",
    "* **Give more explicit instructions.** E.g., if you want the output to be a comma separated list, ask it to return a comma separated list. If you want it to say \"I don't know\" when it doesn't know the answer, tell it 'Say \"I don't know\" if you do not know the answer.'\n",
    "* **Supply better examples.** If you're demonstrating examples in your prompt, make sure that your examples are diverse and high quality.\n",
    "* **Ask the model to answer as if it was an expert.** Explicitly asking the model to produce high quality output or output as if it was written by an expert can induce the model to give higher quality answers that it thinks an expert would write. E.g., \"The following answer is correct, high-quality, and written by an expert.\"\n",
    "* **Prompt the model to write down the series of steps explaining its reasoning.** E.g., prepend your answer with something like \"[Let's think step by step](https://arxiv.org/pdf/2205.11916v1.pdf).\" Prompting the model to give an explanation of its reasoning before its final answer can increase the likelihood that its final answer is consistent and correct.\n",
    "\n",
    "\n",
    "\n",
    "[Fine Tuning Docs]: https://beta.openai.com/docs/guides/fine-tuning\n",
    "[Codex Apps Blog Post]: https://openai.com/blog/codex-apps/\n",
    "[Large language models Blog Post]: https://openai.com/blog/better-language-models/\n",
    "[GitHub Copilot]: https://copilot.github.com/\n",
    "[Codex]: https://openai.com/blog/openai-codex/\n",
    "[GPT3 Apps Blog Post]: https://openai.com/blog/gpt-3-apps/\n",
    "[OpenAI Examples]: https://beta.openai.com/examples\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
