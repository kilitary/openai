{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55905f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install pinecone-client\n",
    "!pip install pandas\n",
    "!pip install typing\n",
    "!pip install tqdm\n",
    "!pip install langchain\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# How to build a tool-using agent with LangChain\n",
    "\n",
    "This notebook takes you through how to use LangChain to augment an OpenAI model with access to external tools. In particular, you'll be able to create LLM agents that use custom tools to answer user queries.\n",
    "\n",
    "\n",
    "## What is Langchain?\n",
    "[LangChain](https://python.langchain.com/en/latest/index.html) is a framework for developing applications powered by language models. Their framework enables you to build layered LLM-powered applications that are context-aware and able to interact dynamically with their environment as agents, leading to simplified code for you and a more dynamic user experience for your customers.\n",
    "\n",
    "## Why do LLMs need to use Tools?\n",
    "One of the most common challenges with LLMs is overcoming the lack of recency and specificity in their training data - answers can be out of date, and they are prone to hallucinations given the huge variety in their knowledge base. Tools are a great method of allowing an LLM to answer within a controlled context that draws on your existing knowledge bases and internal APIs - instead of trying to prompt engineer the LLM all the way to your intended answer, you allow it access to tools that it calls on dynamically for info, parses, and serves to customer.\n",
    "\n",
    "Providing LLMs access to tools can enable them to answer questions with context directly from search engines, APIs or your own databases. Instead of answering directly, an LLM with access to tools can perform intermediate steps to gather relevant information. Tools can also be used in combination. [For example](https://python.langchain.com/en/latest/modules/agents/agents/examples/mrkl_chat.html), a language model can be made to use a search tool to lookup quantitative information and a calculator to execute calculations.\n",
    "\n",
    "## Notebook Sections\n",
    "\n",
    "- **Setup:** Import packages and connect to a Pinecone vector database.\n",
    "- **LLM Agent:** Build an agent that leverages a modified version of the [ReAct](https://react-lm.github.io/) framework to do chain-of-thought reasoning.\n",
    "- **LLM Agent with History:** Provide the LLM with access to previous steps in the conversation.\n",
    "- **Knowledge Base:** Create a knowledge base of \"Stuff You Should Know\" podcast episodes, to be accessed through a tool.\n",
    "- **LLM Agent with Tools:** Extend the agent with access to multiple tools and test that it uses them to answer questions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "\n",
    "Import libraries and set up a connection to a [Pinecone](https://www.pinecone.io) vector database.\n",
    "\n",
    "You can substitute Pinecone for any other vectorstore or database - there are a [selection](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html) that are supported by Langchain natively, while other connectors will need to be developed yourself."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18be3d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "api_key = '4ea18324-3aee-437b-b09b-0e72026cae8b'\n",
    "env = 'gcp-starter'\n",
    "\n",
    "import datetime\n",
    "from pprint import pprint, pp\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import pinecone\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Union\n",
    "import zipfile\n",
    "#from pydantic_v1 import BaseModel, Extra, Field, root_validator\n",
    "# Langchain imports\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.prompts import BaseChatPromptTemplate, ChatPromptTemplate\n",
    "from langchain import SerpAPIWrapper, LLMChain\n",
    "from langchain.schema import AgentAction, AgentFinish, HumanMessage, SystemMessage\n",
    "# LLM wrapper\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import OpenAI\n",
    "# Conversational memory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "# Embeddings and vectorstore\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "# Vectorstore Index\n",
    "index_name = 'dfv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c85f298",
   "metadata": {},
   "source": [
    "For acquiring an API key to connect with Pinecone, you can set up a [free account](https://app.pinecone.io/) and store it in the `api_key` variable below or in your environment variables under `PINECONE_API_KEY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "pinecone.init(api_key=api_key, environment=env)\n",
    "pinecone.whoami()\n",
    "\n",
    "# Check whether the index with the same name already exists - if so, delete it\n",
    "# if index_name in pinecone.list_indexes():\n",
    "#     pinecone.delete_index(index_name)\n",
    "\n",
    "# Creates new index\n",
    "try:\n",
    "    if index_name not in pinecone.list_indexes():\n",
    "        pinecone.create_index(name=index_name, dimension=1536)\n",
    "        print('created: ', index_name)\n",
    "    index = pinecone.Index(index_name=index_name)\n",
    "except Exception as e:\n",
    "    print(f'pinecone: {e}')\n",
    "\n",
    "# Confirm our index was created\n",
    "pinecone.list_indexes()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "7e7644cd",
   "metadata": {},
   "source": [
    "Run this code block if you want to clear the index, or if the index doesn't exist yet\n",
    "\n",
    "```\n",
    "# Check whether the index with the same name already exists - if so, delete it\n",
    "if index_name in pinecone.list_indexes():\n",
    "    pinecone.delete_index(index_name)\n",
    "\n",
    "# Creates new index\n",
    "pinecone.create_index(name=index_name, dimension=1536)\n",
    "index = pinecone.Index(index_name=index_name)\n",
    "\n",
    "# Confirm our index was created\n",
    "pinecone.list_indexes()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb63e37",
   "metadata": {},
   "source": [
    "## LLM Agent\n",
    "\n",
    "An [LLM agent](https://python.langchain.com/en/latest/modules/agents/agents/custom_llm_agent.html) in Langchain has many configurable components, which are detailed in the Langchain documentation.\n",
    "\n",
    "We'll employ a few of the core concepts to make an agent that talks in the way we want, can use tools to answer questions, and uses the appropriate language model to power the conversation.\n",
    "- **Prompt Template:** The input template to control the LLM's behaviour and how it accepts inputs and produces outputs - this is the brain that drives your application ([docs](https://python.langchain.com/en/latest/modules/prompts/prompt_templates.html)).\n",
    "- **Output Parser:** A method of parsing the output from the prompt. If the LLM produces output using certain headers, you can enable complex interactions where variables are generated by the LLM in their response and passed into the next step of the chain ([docs](https://python.langchain.com/en/latest/modules/prompts/output_parsers.html)).\n",
    "- **LLM Chain:** A Chain brings together a prompt template with an LLM that will execute it - in this case we'll be using ```gpt-3.5-turbo``` but this framework can be used with OpenAI completions models, or other LLMs entirely ([docs](https://python.langchain.com/en/latest/modules/chains.html)).\n",
    "- **Tool:** An external service that the LLM can use to retrieve information or execute commands should the user require it ([docs](https://python.langchain.com/en/latest/modules/agents/tools.html)).\n",
    "- **Agent:** The glue that brings all of this together, an agent can call multiple LLM Chains, each with their own tools. Agents can be extended with your own logic to allow retries, error handling and any other methods you choose to add reliability to your application ([docs](https://python.langchain.com/en/latest/modules/agents.html)).\n",
    "\n",
    "**NB:** Before using this cookbook with the Search tool you'll need to sign up on https://serpapi.com/ and generate an API key. Once you have it, store it in an environment variable named ```SERPAPI_API_KEY```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tool(name='connect', description='network connect with the host port', args_schema=None, return_direct=False, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, handle_tool_error=True, func=StructuredTool(name='connect', description='connect(host: str, port: int) -> str - Searches the API for the query.', args_schema=<class 'pydantic.main.connectSchemaSchema'>, return_direct=False, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, handle_tool_error=False, func=<function connect_api at 0x0000017191683670>, coroutine=None), coroutine=None),\n",
      " Tool(name='connect', description='download file', args_schema=None, return_direct=False, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, handle_tool_error=True, func=StructuredTool(name='fetch', description='fetch(url: str) -> str - fetch url', args_schema=<class 'pydantic.main.fetchSchemaSchema'>, return_direct=False, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, handle_tool_error=False, func=<function fetch_url at 0x000001719161BDC0>, coroutine=None), coroutine=None)]\n"
     ]
    }
   ],
   "source": [
    "# Initiate a Search tool - note you'll need to have set SERPAPI_API_KEY as an environment variable as per the above instructions\n",
    "# search = SerpAPIWrapper()\n",
    "#\n",
    "# class FunctionTool(BaseModel):\n",
    "#     def run(self, query: str) -> str:\n",
    "#         print(f'FunctionTool Item: {query}')\n",
    "#         return f\"answer: {bytes(m).hex().capitalize()}\"\n",
    "#\n",
    "import requests\n",
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "@tool('connect', return_direct=False)\n",
    "def connect_api(host: str, port: int) -> str:\n",
    "    \"\"\"Searches the API for the query.\"\"\"\n",
    "    print(f'{connect_api: {host}:{port}}')\n",
    "    results = f'Host: {host}\\r\\n\\r\\n'\n",
    "    return f\"reply at {host}:{port}: {results}\"\n",
    "\n",
    "\n",
    "@tool('fetch')\n",
    "def fetch_url(url: str) -> str:\n",
    "    \"\"\"fetch url\"\"\"\n",
    "    result = requests.post(url)\n",
    "    return f\"Status: {result.status_code} - {result.text}\"\n",
    "\n",
    "\n",
    "# Define a list of tools\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        func=connect_api,\n",
    "        name=\"connect\",\n",
    "        description='network connect with the host port',\n",
    "        handle_tool_error=True,\n",
    "    ),\n",
    "]\n",
    "\n",
    "tools.append(\n",
    "    Tool.from_function(\n",
    "        func=fetch_url,\n",
    "        name=\"connect\",\n",
    "        description='download file',\n",
    "        handle_tool_error=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "pprint(tools, compact=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T16:31:29.216192500Z",
     "start_time": "2023-09-16T16:31:29.090196500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "effae642",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-16T16:31:31.897564900Z",
     "start_time": "2023-09-16T16:31:31.770568400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up the prompt with input variables for tools, user input and a scratchpad for the model to record its workings\n",
    "template = \"\"\"Answer the following questions as best you can, but speaking as a teacher might speak. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin! Remember to speak as a teacher when giving your final answer.\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Set up a prompt template\n",
    "class CustomPromptTemplate(BaseChatPromptTemplate):\n",
    "    # The template to use\n",
    "    template: str\n",
    "    # The list of tools available\n",
    "    tools: List[Tool]\n",
    "\n",
    "    def format_messages(self, **kwargs) -> str:\n",
    "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
    "\n",
    "        # Format them in a particular way\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "\n",
    "        # Set the agent_scratchpad variable to that value\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "\n",
    "        # Create a tools variable from the list of tools provided\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
    "\n",
    "        # Create a list of tool names for the tools provided\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
    "        formatted = self.template.format(**kwargs)\n",
    "        print(f'formatted templtate:\\n---\\n{formatted}\\n---')\n",
    "        return [HumanMessage(content=formatted)]\n",
    "\n",
    "\n",
    "prompt = CustomPromptTemplate(\n",
    "    template=template,\n",
    "    tools=tools,\n",
    "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
    "    # This includes the `intermediate_steps` variable because that is needed\n",
    "    input_variables=[\"input\", \"intermediate_steps\"]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T16:31:33.931114800Z",
     "start_time": "2023-09-16T16:31:33.794112100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "533657a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-16T16:31:36.451831500Z",
     "start_time": "2023-09-16T16:31:36.330831200Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomOutputParser(AgentOutputParser):\n",
    "\n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "\n",
    "        # Check if agent should finish\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            return AgentFinish(\n",
    "                # Return values is generally always a dictionary with a single `output` key\n",
    "                # It is not recommended to try anything else at the moment :)\n",
    "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "\n",
    "        # Parse out the action and action input\n",
    "        regex = r\"Action: (.*?)[\\n]*Action Input:[\\s]*(.*)\"\n",
    "        match = re.search(regex, llm_output, re.DOTALL)\n",
    "\n",
    "        # If it can't parse the output it raises an error\n",
    "        # You can add your own logic here to handle errors in a different way i.e. pass to a human, give a canned response\n",
    "        if not match:\n",
    "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "        action = match.group(1).strip()\n",
    "        action_input = match.group(2)\n",
    "\n",
    "        # Return the action and action input\n",
    "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n",
    "\n",
    "\n",
    "output_parser = CustomOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c7d93d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-16T16:31:38.061863300Z",
     "start_time": "2023-09-16T16:31:37.917871300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initiate our LLM - default is 'gpt-3.5-turbo'\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# LLM chain consisting of the LLM and a prompt\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Using tools, the LLM chain and output_parser to make an agent\n",
    "tool_names = [tool.name for tool in tools]\n",
    "\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    # We use \"Observation\" as our stop sequence so it will stop when it receives Tool output\n",
    "    # If you change your prompt template you'll need to adjust this as well\n",
    "    stop=[\"\\nObservation:\"],\n",
    "    allowed_tools=tool_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd893fa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-16T16:31:39.757002300Z",
     "start_time": "2023-09-16T16:31:39.595452100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initiate the agent that will respond to our queries\n",
    "# Set verbose=True to share the CoT reasoning the LLM goes through\n",
    "#print(tools)\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7ca3ee2",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-09-16T16:35:21.014477100Z",
     "start_time": "2023-09-16T16:35:08.254743300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "formatted templtate:\n",
      "---\n",
      "Answer the following questions as best you can, but speaking as a teacher might speak. You have access to the following tools:\n",
      "\n",
      "connect: network connect with the host port\n",
      "connect: download file\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [connect, connect]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin! Remember to speak as a teacher when giving your final answer.\n",
      "\n",
      "Question: count how many planets humans have?\n",
      "\n",
      "---\n",
      "\u001B[32;1m\u001B[1;3mThought: To answer this question, I need to access information about the number of planets humans have.\n",
      "Action: Connect to the network and search for reliable sources of information.\n",
      "Action Input: connect: network connect with the host port\u001B[0m\n",
      "\n",
      "Observation:Connect to the network and search for reliable sources of information. is not a valid tool, try one of [connect].\n",
      "formatted templtate:\n",
      "---\n",
      "Answer the following questions as best you can, but speaking as a teacher might speak. You have access to the following tools:\n",
      "\n",
      "connect: network connect with the host port\n",
      "connect: download file\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [connect, connect]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin! Remember to speak as a teacher when giving your final answer.\n",
      "\n",
      "Question: count how many planets humans have?\n",
      "Thought: To answer this question, I need to access information about the number of planets humans have.\n",
      "Action: Connect to the network and search for reliable sources of information.\n",
      "Action Input: connect: network connect with the host port\n",
      "Observation: Connect to the network and search for reliable sources of information. is not a valid tool, try one of [connect].\n",
      "Thought: \n",
      "---\n",
      "\u001B[32;1m\u001B[1;3mI need to download a file that contains information about the number of planets humans have.\n",
      "Action: Connect to the network and download the file.\n",
      "Action Input: connect: download file\u001B[0m\n",
      "\n",
      "Observation:Connect to the network and download the file. is not a valid tool, try one of [connect].\n",
      "formatted templtate:\n",
      "---\n",
      "Answer the following questions as best you can, but speaking as a teacher might speak. You have access to the following tools:\n",
      "\n",
      "connect: network connect with the host port\n",
      "connect: download file\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [connect, connect]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin! Remember to speak as a teacher when giving your final answer.\n",
      "\n",
      "Question: count how many planets humans have?\n",
      "Thought: To answer this question, I need to access information about the number of planets humans have.\n",
      "Action: Connect to the network and search for reliable sources of information.\n",
      "Action Input: connect: network connect with the host port\n",
      "Observation: Connect to the network and search for reliable sources of information. is not a valid tool, try one of [connect].\n",
      "Thought: I need to download a file that contains information about the number of planets humans have.\n",
      "Action: Connect to the network and download the file.\n",
      "Action Input: connect: download file\n",
      "Observation: Connect to the network and download the file. is not a valid tool, try one of [connect].\n",
      "Thought: \n",
      "---\n",
      "\u001B[32;1m\u001B[1;3mI can use the internet to search for reliable sources of information about the number of planets humans have. \n",
      "Action: Connect to the network and search for reliable sources of information.\n",
      "Action Input: connect: network connect with the host port\u001B[0m\n",
      "\n",
      "Observation:Connect to the network and search for reliable sources of information. is not a valid tool, try one of [connect].\n",
      "formatted templtate:\n",
      "---\n",
      "Answer the following questions as best you can, but speaking as a teacher might speak. You have access to the following tools:\n",
      "\n",
      "connect: network connect with the host port\n",
      "connect: download file\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [connect, connect]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin! Remember to speak as a teacher when giving your final answer.\n",
      "\n",
      "Question: count how many planets humans have?\n",
      "Thought: To answer this question, I need to access information about the number of planets humans have.\n",
      "Action: Connect to the network and search for reliable sources of information.\n",
      "Action Input: connect: network connect with the host port\n",
      "Observation: Connect to the network and search for reliable sources of information. is not a valid tool, try one of [connect].\n",
      "Thought: I need to download a file that contains information about the number of planets humans have.\n",
      "Action: Connect to the network and download the file.\n",
      "Action Input: connect: download file\n",
      "Observation: Connect to the network and download the file. is not a valid tool, try one of [connect].\n",
      "Thought: I can use the internet to search for reliable sources of information about the number of planets humans have. \n",
      "Action: Connect to the network and search for reliable sources of information.\n",
      "Action Input: connect: network connect with the host port\n",
      "Observation: Connect to the network and search for reliable sources of information. is not a valid tool, try one of [connect].\n",
      "Thought: \n",
      "---\n",
      "\u001B[32;1m\u001B[1;3mI can use the internet to search for reliable sources of information about the number of planets humans have. \n",
      "Action: Connect to the network and search for reliable sources of information.\n",
      "Action Input: connect: network connect with the host port\u001B[0m\n",
      "\n",
      "Observation:Connect to the network and search for reliable sources of information. is not a valid tool, try one of [connect].\n",
      "formatted templtate:\n",
      "---\n",
      "Answer the following questions as best you can, but speaking as a teacher might speak. You have access to the following tools:\n",
      "\n",
      "connect: network connect with the host port\n",
      "connect: download file\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [connect, connect]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin! Remember to speak as a teacher when giving your final answer.\n",
      "\n",
      "Question: count how many planets humans have?\n",
      "Thought: To answer this question, I need to access information about the number of planets humans have.\n",
      "Action: Connect to the network and search for reliable sources of information.\n",
      "Action Input: connect: network connect with the host port\n",
      "Observation: Connect to the network and search for reliable sources of information. is not a valid tool, try one of [connect].\n",
      "Thought: I need to download a file that contains information about the number of planets humans have.\n",
      "Action: Connect to the network and download the file.\n",
      "Action Input: connect: download file\n",
      "Observation: Connect to the network and download the file. is not a valid tool, try one of [connect].\n",
      "Thought: I can use the internet to search for reliable sources of information about the number of planets humans have. \n",
      "Action: Connect to the network and search for reliable sources of information.\n",
      "Action Input: connect: network connect with the host port\n",
      "Observation: Connect to the network and search for reliable sources of information. is not a valid tool, try one of [connect].\n",
      "Thought: I can use the internet to search for reliable sources of information about the number of planets humans have. \n",
      "Action: Connect to the network and search for reliable sources of information.\n",
      "Action Input: connect: network connect with the host port\n",
      "Observation: Connect to the network and search for reliable sources of information. is not a valid tool, try one of [connect].\n",
      "Thought: \n",
      "---\n",
      "\u001B[32;1m\u001B[1;3mI can use the internet to search for reliable sources of information about the number of planets humans have. \n",
      "Action: Connect to the network and search for reliable sources of information.\n",
      "Action Input: connect: network connect with the host port\u001B[0m\n",
      "\n",
      "Observation:Connect to the network and search for reliable sources of information. is not a valid tool, try one of [connect].\n",
      "formatted templtate:\n",
      "---\n",
      "Answer the following questions as best you can, but speaking as a teacher might speak. You have access to the following tools:\n",
      "\n",
      "connect: network connect with the host port\n",
      "connect: download file\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [connect, connect]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin! Remember to speak as a teacher when giving your final answer.\n",
      "\n",
      "Question: count how many planets humans have?\n",
      "Thought: To answer this question, I need to access information about the number of planets humans have.\n",
      "Action: Connect to the network and search for reliable sources of information.\n",
      "Action Input: connect: network connect with the host port\n",
      "Observation: Connect to the network and search for reliable sources of information. is not a valid tool, try one of [connect].\n",
      "Thought: I need to download a file that contains information about the number of planets humans have.\n",
      "Action: Connect to the network and download the file.\n",
      "Action Input: connect: download file\n",
      "Observation: Connect to the network and download the file. is not a valid tool, try one of [connect].\n",
      "Thought: I can use the internet to search for reliable sources of information about the number of planets humans have. \n",
      "Action: Connect to the network and search for reliable sources of information.\n",
      "Action Input: connect: network connect with the host port\n",
      "Observation: Connect to the network and search for reliable sources of information. is not a valid tool, try one of [connect].\n",
      "Thought: I can use the internet to search for reliable sources of information about the number of planets humans have. \n",
      "Action: Connect to the network and search for reliable sources of information.\n",
      "Action Input: connect: network connect with the host port\n",
      "Observation: Connect to the network and search for reliable sources of information. is not a valid tool, try one of [connect].\n",
      "Thought: I can use the internet to search for reliable sources of information about the number of planets humans have. \n",
      "Action: Connect to the network and search for reliable sources of information.\n",
      "Action Input: connect: network connect with the host port\n",
      "Observation: Connect to the network and search for reliable sources of information. is not a valid tool, try one of [connect].\n",
      "Thought: \n",
      "---\n",
      "\u001B[32;1m\u001B[1;3mI now know the final answer.\n",
      "Final Answer: As a teacher, I would advise my students to use the internet to search for reliable sources of information about the number of planets humans have.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "'As a teacher, I would advise my students to use the internet to search for reliable sources of information about the number of planets humans have.'"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#agent_executor.run(\"How many people live in canada as of 2023?\")\n",
    "agent_executor.run(\"count how many planets humans have?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "formatted templtate:\n",
      "---\n",
      "Answer the following questions as best you can, but speaking as a teacher might speak. You have access to the following tools:\n",
      "\n",
      "connect: network connect with the host port\n",
      "connect: download file\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [connect, connect]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin! Remember to speak as a teacher when giving your final answer.\n",
      "\n",
      "Question: what is on https://ya.ru/\n",
      "\n",
      "---\n",
      "\u001B[32;1m\u001B[1;3mThought: To find out what is on a website, we need to connect to it and access its content.\n",
      "Action: connect\n",
      "Action Input: https://ya.ru/\u001B[0m\n",
      "\n",
      "Observation:\u001B[33;1m\u001B[1;3mStatus: 200 - <!doctype html><html prefix=\"og: http://ogp.me/ns#\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"><meta charset=\"utf-8\"><meta name=\"viewport\" content=\"width=device-width,initial-scale=1\"><title data-react-helmet=\"true\">ÐÐ¹!</title><meta data-react-helmet=\"true\" property=\"og:title\" content=\"Ð¯Ð½Ð´ÐµÐºÑ\"><meta data-react-helmet=\"true\" property=\"og:description\" content=\"ÐÐ°Ð¹Ð´ÑÑÑÑ Ð²ÑÑ\"><meta data-react-helmet=\"true\" property=\"og:image\" content=\"https://yastatic.net/s3/home-static/_/37/37a02b5dc7a51abac55d8a5b6c865f0e.png\"><link rel=\"stylesheet\" href=\"/captcha_smart.a737a5c9dbdd19f5b829.min.css?k=1688990308309\"><style>@media only screen and (min-width:651px){body{background-image:url('https://captcha-backgrounds.s3.yandex.net/static/default-background.jpg')}}.LogoLink{background-image:url('data:image/svg+xml;base64,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')}</style><div id=\"root\"><div class=\"Theme Theme_color_yandex-default Theme_root_default\"><div class=\"Container\"><div class=\"Spacer\" style=\"padding-bottom:40px\"><a href=\"https://www.ya.ru\" title=\"Yandex\" class=\"Link Link_view_default LogoLink\"></a></div><form method=\"POST\" action=\"/checkcaptcha?key=e0b2cfe6-cbffca83-d1c8c-2ecc9be_2/1694881925/959657f24cad8037b858d37b4440b092_7a6fe709ed77d9c1ebe8bd8579b0e818&mt=2B60511570234A6F73DB7CE73B3817944EFDBBE5C7092515270ED26B239E1BFD52F698F441BD4F4602FE459AD81B1924BE1ADF086114AE6209AA8F99E41FA677F920D39B821DD7633F9C0A7BABF98400BAA0A66493921B286749C2414090CAA6163F7D90BF82703ACCEF6DFDE767&retpath=aHR0cHM6Ly95YS5ydS8__cb65b4c42c6b3840def67fee2954e15d&u=a672f7af-6d19924a-a4b6b1a0-9777cc28&s=b823da6d18a9c47d40e7bc2c6185de87\" id=\"checkbox-captcha-form\"><div class=\"Spacer\" style=\"padding-bottom:16px\"><span class=\"Text Text_weight_medium Text_typography_headline-s\">ÐÐ¾Ð´ÑÐ²ÐµÑÐ´Ð¸ÑÐµ, ÑÑÐ¾ Ð·Ð°Ð¿ÑÐ¾ÑÑ Ð¾ÑÐ¿ÑÐ°Ð²Ð»ÑÐ»Ð¸ Ð²Ñ, Ð° Ð½Ðµ ÑÐ¾Ð±Ð¾Ñ</span></div><div class=\"Spacer\" style=\"padding-bottom:16px\"><span class=\"Text Text_weight_regular Text_typography_body-long-m\">ÐÐ°Ð¼ Ð¾ÑÐµÐ½Ñ Ð¶Ð°Ð»Ñ, Ð½Ð¾ Ð·Ð°Ð¿ÑÐ¾ÑÑ Ñ Ð²Ð°ÑÐµÐ³Ð¾ ÑÑÑÑÐ¾Ð¹ÑÑÐ²Ð° Ð¿Ð¾ÑÐ¾Ð¶Ð¸ Ð½Ð° Ð°Ð²ÑÐ¾Ð¼Ð°ÑÐ¸ÑÐµÑÐºÐ¸Ðµ.Â  Â <a href=\"https://yandex.ru/support/smart-captcha?unique_key=a672f7af-6d19924a-a4b6b1a0-9777cc28\" class=\"Link Link_view_default\">ÐÐ¾ÑÐµÐ¼Ñ ÑÑÐ¾ Ð¼Ð¾Ð³Ð»Ð¾ Ð¿ÑÐ¾Ð¸Ð·Ð¾Ð¹ÑÐ¸?</a></span></div><noscript><span class=\"Text Text_color_alert Text_weight_medium Text_typography_body-long-m\">Ð£ Ð²Ð°Ñ Ð¾ÑÐºÐ»ÑÑÐµÐ½Ð¾ Ð¸ÑÐ¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ JavaScript. ÐÐ¾ Ð½Ð°Ð¶Ð°ÑÐ¸Ñ Ð²Ñ Ð±ÑÐ´ÐµÑÐµ Ð½Ð°Ð¿ÑÐ°Ð²Ð»ÐµÐ½Ñ Ð½Ð° Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸ÑÐµÐ»ÑÐ½ÑÑ Ð¿ÑÐ¾Ð²ÐµÑÐºÑ.Â <a href=\"https://yandex.ru/support/common/browsers-settings/browsers-java-js-settings.html\" class=\"Link Link_view_default\">ÐÐ°Ðº Ð²ÐºÐ»ÑÑÐ¸ÑÑ JavaScript?</a></span></noscript><div class=\"Spacer Spacer_auto-gap_bottom\" style=\"padding-top:40px;padding-bottom:40px\"><div class=\"CheckboxCaptcha\" data-testid=\"checkbox-captcha\"><div class=\"CheckboxCaptcha-Inner\"><div class=\"CheckboxCaptcha-Anchor\"><input type=\"button\" id=\"js-button\" class=\"CheckboxCaptcha-Button\" aria-checked=\"false\" aria-labelledby=\"smartcaptcha-status\" role=\"checkbox\"><noscript><input type=\"submit\" class=\"CheckboxCaptcha-Button\" aria-checked=\"false\" aria-labelledby=\"smartcaptcha-status\" role=\"checkbox\"></noscript><div class=\"CheckboxCaptcha-Checkbox\" data-checked=\"false\"><svg class=\"SvgIcon\" width=\"24\" height=\"24\" viewBox=\"0 0 24 25\" fill=\"none\"><path d=\"M4 12.5L9.5 18.5L20 6.5\" stroke=\"#000\" stroke-width=\"3\" stroke-linecap=\"round\" stroke-linejoin=\"round\"></path></svg></div></div><div class=\"CheckboxCaptcha-Label\"><span class=\"Text Text_weight_regular Text_typography_control-xxl CheckboxCaptcha-LabelText\">Ð¯ Ð½Ðµ ÑÐ¾Ð±Ð¾Ñ</span><span class=\"Text Text_color_control-secondary Text_weight_regular Text_typography_control-l CheckboxCaptcha-SecondaryText\">ÐÐ°Ð¶Ð¼Ð¸ÑÐµ, ÑÑÐ¾Ð±Ñ Ð¿ÑÐ¾Ð´Ð¾Ð»Ð¶Ð¸ÑÑ</span><span class=\"Text Text_color_ghost Text_weight_regular Text_typography_control-s CheckboxCaptcha-Links\"><a target=\"_blank\" href=\"https://cloud.yandex.ru/services/smartcaptcha\" aria-describedby=\"service-link-description\" class=\"Link Link_view_captcha\">Yandex SmartCaptcha</a><span class=\"Links-Dot\">â¢</span><a target=\"_blank\" href=\"https://yandex.ru/legal/smartcaptcha_notice/\" aria-describedby=\"connect-link-description\" class=\"Link Link_view_captcha\">ÐÐ±ÑÐ°Ð±Ð¾ÑÐºÐ° Ð´Ð°Ð½Ð½ÑÑ</a><div class=\"visuallyhidden\" id=\"service-link-description\">connect-link-description</div><div class=\"visuallyhidden\" id=\"privacy-link-description\">Ð¡Ð°Ð¹Ñ Ñ Ð¸Ð½ÑÐ¾ÑÐ¼Ð°ÑÐ¸ÐµÐ¹ Ð¾Ð± Ð¾Ð±ÑÐ°Ð±Ð¾ÑÐºÐµ Ð´Ð°Ð½Ð½ÑÑ</div></span></div><div class=\"CheckboxCaptcha-Icon\"><svg width=\"41\" height=\"20\" viewBox=\"0 0 41 20\" fill=\"none\" class=\"CloudIcon\"><path d=\"M10.251 11.0479C10.7785 12.2034 10.9542 12.6053 10.9542 13.9932V15.8333H9.07031V12.7309L5.51562 5H7.48142L10.251 11.0479ZM12.5746 5L10.2698 10.2376H12.1852L14.4963 5H12.5746Z\" fill=\"#000\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M10 18.2292C14.5448 18.2292 18.2292 14.5448 18.2292 10C18.2292 5.45516 14.5448 1.77083 10 1.77083C5.45516 1.77083 1.77083 5.45516 1.77083 10C1.77083 14.5448 5.45516 18.2292 10 18.2292ZM10 20C15.5228 20 20 15.5228 20 10C20 4.47715 15.5228 0 10 0C4.47715 0 0 4.47715 0 10C0 15.5228 4.47715 20 10 20Z\" fill=\"#000\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M39.2333 10C39.2333 14.55 35.55 18.2333 31 18.2333C29.8094 18.2333 28.9202 17.7864 28.4755 17.2443C28.0308 16.7022 27.9057 15.8152 28.0588 14.5339L30.7088 14.0082C33.13 13.5647 34.4641 12.2176 34.9417 9.80288C35.0001 9.46485 35.0669 9.12682 35.1301 8.8064C35.156 8.67525 35.1813 8.54705 35.2052 8.42301C35.2774 8.03104 35.3471 7.65855 35.411 7.31691C35.4783 6.95699 35.5392 6.63132 35.5899 6.3532C35.861 4.78329 35.6407 3.62423 35.0309 2.81788C37.5398 4.22801 39.2333 6.91477 39.2333 10ZM31 1.76667C26.45 1.76667 22.7667 5.45 22.7667 10C22.7667 13.0852 24.4602 15.772 26.9691 17.1821C26.3593 16.3758 26.139 15.2167 26.4101 13.6468C26.4684 13.3273 26.538 12.9507 26.6158 12.5305C26.67 12.2378 26.7281 11.9239 26.7889 11.5934C26.8128 11.4694 26.8381 11.3412 26.864 11.21C26.9273 10.8896 26.994 10.5516 27.0524 10.2136C27.5136 7.78235 28.8477 6.43534 31.2688 5.99181L33.9412 5.46614C34.0943 4.18484 33.9692 3.29778 33.5245 2.75569C33.0798 2.2136 32.1906 1.76667 31 1.76667ZM41 10C41 15.5167 36.5167 20 31 20C25.4833 20 21 15.5167 21 10C21 4.48333 25.4833 0 31 0C36.5167 0 41 4.48333 41 10ZM33.6241 7.30597L31.6147 7.70022C29.9018 8.02876 29.1112 8.81726 28.7818 10.5257C28.7052 10.9076 28.6385 11.2698 28.5741 11.6199C28.5554 11.7215 28.5368 11.8222 28.5183 11.922C28.4689 12.1848 28.4195 12.4476 28.3701 12.694L30.363 12.2998C32.0759 11.9712 32.883 11.1827 33.2124 9.47434C33.2848 9.11359 33.3515 8.75192 33.4182 8.39024C33.485 8.02852 33.5517 7.6668 33.6241 7.30597Z\" fill=\"#000\"></path></svg></div><div class=\"CheckboxCaptcha-Alert\"></div><div id=\"smartcaptcha-status\" class=\"CheckboxCaptcha-Status visuallyhidden\">SmartCaptcha Ð½ÑÐ¶Ð½Ð° Ð¿ÑÐ¾Ð²ÐµÑÐºÐ° Ð¿Ð¾Ð»ÑÐ·Ð¾Ð²Ð°ÑÐµÐ»Ñ</div></div></div></div><span class=\"Text Text_color_ghost Text_weight_regular Text_typography_control-xs\">ÐÑÐ»Ð¸ Ñ Ð²Ð°Ñ Ð²Ð¾Ð·Ð½Ð¸ÐºÐ»Ð¸ Ð¿ÑÐ¾Ð±Ð»ÐµÐ¼Ñ, Ð¿Ð¾Ð¶Ð°Ð»ÑÐ¹ÑÑÐ°, Ð²Ð¾ÑÐ¿Ð¾Ð»ÑÐ·ÑÐ¹ÑÐµÑÑÂ <a href=\"https://yandex.ru/support/smart-captcha?unique_key=a672f7af-6d19924a-a4b6b1a0-9777cc28#help\" class=\"Link Link_view_default\">ÑÐ¾ÑÐ¼Ð¾Ð¹ Ð¾Ð±ÑÐ°ÑÐ½Ð¾Ð¹ ÑÐ²ÑÐ·Ð¸</a></span><input type=\"hidden\" name=\"rdata\"><input type=\"hidden\" name=\"aesKey\"><input type=\"hidden\" name=\"signKey\"><input type=\"hidden\" name=\"pdata\"><input type=\"hidden\" name=\"tdata\"></form></div></div></div><script>const button=document.getElementById(\"js-button\");button.addEventListener(\"click\",function n(t){window.__JS_BUTTON_CLICKED__=!0,this.removeEventListener(\"click\",n,!1)},!1),window.onerror=function(n,t){0===t.indexOf(window.location.origin+\"/captcha_smart.a737a5c9dbdd19f5b829.min.js\")&&(button.type=\"submit\",window.Ya&&window.Ya.Rum&&window.Ya.Rum.logEventString&&window.Ya.Rum.logEventString(\"js_fail_force_submit_type\",n,{page:\"checkbox\"}))}</script> <script>window.__SSR_DATA__={url:\"/ru/checkbox\",invalid:\"no\",formAction:\"/checkcaptcha?key=e0b2cfe6-cbffca83-d1c8c-2ecc9be_2/1694881925/959657f24cad8037b858d37b4440b092_7a6fe709ed77d9c1ebe8bd8579b0e818&mt=2B60511570234A6F73DB7CE73B3817944EFDBBE5C7092515270ED26B239E1BFD52F698F441BD4F4602FE459AD81B1924BE1ADF086114AE6209AA8F99E41FA677F920D39B821DD7633F9C0A7BABF98400BAA0A66493921B286749C2414090CAA6163F7D90BF82703ACCEF6DFDE767&retpath=aHR0cHM6Ly95YS5ydS8__cb65b4c42c6b3840def67fee2954e15d&u=a672f7af-6d19924a-a4b6b1a0-9777cc28&s=b823da6d18a9c47d40e7bc2c6185de87\",captchaKey:\"e0b2cfe6-cbffca83-d1c8c-2ecc9be_2/1694881925/959657f24cad8037b858d37b4440b092_7a6fe709ed77d9c1ebe8bd8579b0e818\",imageSrc:\"\",taskImageSrc:\"\",voiceSrc:\"\",introSrc:\"\",aesKey:\"zmc9gKBUBaOSZ7r/xUXXQSq8my/GfOewX6L3MY9hNbg=\",aesSign:\"1_1694881925_2907227557850372060_ae1f7531693d592fe71700ec3ae47348\",reqId:\"1694881925588248-8784055439225661181-balancer-l7leveler-kubr-yp-vla-99-BAL\",uniqueKey:\"a672f7af-6d19924a-a4b6b1a0-9777cc28\",powPrefix:\"743D313639343838313932353B703D32613830623864652D34303762326131382D62666531393433392D36623735383737633B633D31353B643D39393037433643363746463633313141463237413230393335443039313633323B\",powComplexity:\"15\"}</script><script src=\"/captcha_smart_error.a737a5c9dbdd19f5b829.min.js?k=1688990308309\" crossorigin=\"\"></script><script src=\"https://yastatic.net/react/16.8.4/react-with-dom-and-polyfills.min.js\" crossorigin=\"\"></script><script src=\"/captcha_smart.a737a5c9dbdd19f5b829.min.js?k=1688990308309\" crossorigin=\"\"></script><script>!function(e,n,t,a,c){e.ym=e.ym||function(){(e.ym.a=e.ym.a||[]).push(arguments)},e.ym.l=+new Date,a=n.createElement(t),c=n.getElementsByTagName(t)[0],a.async=1,a.src=\"https://mc.yandex.ru/metrika/tag.js\",c.parentNode.insertBefore(a,c)}(window,document,\"script\"),ym(10630330,\"init\",{clickmap:!0,trackLinks:!0,accurateTrackBounce:!0,webvisor:!0,ut:\"noindex\",params:{req_id:\"1694881925588248-8784055439225661181-balancer-l7leveler-kubr-yp-vla-99-BAL\",unique_key:\"a672f7af-6d19924a-a4b6b1a0-9777cc28\"}})</script><noscript><div><img src=\"https://mc.yandex.ru/watch/10630330?ut=noindex\" style=\"position:absolute;left:-9999px\" alt=\"\"></div></noscript><div><img src=\"https://adfstat.yandex.ru/captcha?req_id=1694881925588248-8784055439225661181-balancer-l7leveler-kubr-yp-vla-99-BAL&unique_key=a672f7af-6d19924a-a4b6b1a0-9777cc28\" style=\"position:absolute;left:-9999px\" alt=\"\"></div>\u001B[0m\n",
      "formatted templtate:\n",
      "---\n",
      "Answer the following questions as best you can, but speaking as a teacher might speak. You have access to the following tools:\n",
      "\n",
      "connect: network connect with the host port\n",
      "connect: download file\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [connect, connect]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin! Remember to speak as a teacher when giving your final answer.\n",
      "\n",
      "Question: what is on https://ya.ru/\n",
      "Thought: To find out what is on a website, we need to connect to it and access its content.\n",
      "Action: connect\n",
      "Action Input: https://ya.ru/\n",
      "Observation: Status: 200 - <!doctype html><html prefix=\"og: http://ogp.me/ns#\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"><meta charset=\"utf-8\"><meta name=\"viewport\" content=\"width=device-width,initial-scale=1\"><title data-react-helmet=\"true\">ÐÐ¹!</title><meta data-react-helmet=\"true\" property=\"og:title\" content=\"Ð¯Ð½Ð´ÐµÐºÑ\"><meta data-react-helmet=\"true\" property=\"og:description\" content=\"ÐÐ°Ð¹Ð´ÑÑÑÑ Ð²ÑÑ\"><meta data-react-helmet=\"true\" property=\"og:image\" content=\"https://yastatic.net/s3/home-static/_/37/37a02b5dc7a51abac55d8a5b6c865f0e.png\"><link rel=\"stylesheet\" href=\"/captcha_smart.a737a5c9dbdd19f5b829.min.css?k=1688990308309\"><style>@media only screen and (min-width:651px){body{background-image:url('https://captcha-backgrounds.s3.yandex.net/static/default-background.jpg')}}.LogoLink{background-image:url('data:image/svg+xml;base64,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')}</style><div id=\"root\"><div class=\"Theme Theme_color_yandex-default Theme_root_default\"><div class=\"Container\"><div class=\"Spacer\" style=\"padding-bottom:40px\"><a href=\"https://www.ya.ru\" title=\"Yandex\" class=\"Link Link_view_default LogoLink\"></a></div><form method=\"POST\" action=\"/checkcaptcha?key=e0b2cfe6-cbffca83-d1c8c-2ecc9be_2/1694881925/959657f24cad8037b858d37b4440b092_7a6fe709ed77d9c1ebe8bd8579b0e818&mt=2B60511570234A6F73DB7CE73B3817944EFDBBE5C7092515270ED26B239E1BFD52F698F441BD4F4602FE459AD81B1924BE1ADF086114AE6209AA8F99E41FA677F920D39B821DD7633F9C0A7BABF98400BAA0A66493921B286749C2414090CAA6163F7D90BF82703ACCEF6DFDE767&retpath=aHR0cHM6Ly95YS5ydS8__cb65b4c42c6b3840def67fee2954e15d&u=a672f7af-6d19924a-a4b6b1a0-9777cc28&s=b823da6d18a9c47d40e7bc2c6185de87\" id=\"checkbox-captcha-form\"><div class=\"Spacer\" style=\"padding-bottom:16px\"><span class=\"Text Text_weight_medium Text_typography_headline-s\">ÐÐ¾Ð´ÑÐ²ÐµÑÐ´Ð¸ÑÐµ, ÑÑÐ¾ Ð·Ð°Ð¿ÑÐ¾ÑÑ Ð¾ÑÐ¿ÑÐ°Ð²Ð»ÑÐ»Ð¸ Ð²Ñ, Ð° Ð½Ðµ ÑÐ¾Ð±Ð¾Ñ</span></div><div class=\"Spacer\" style=\"padding-bottom:16px\"><span class=\"Text Text_weight_regular Text_typography_body-long-m\">ÐÐ°Ð¼ Ð¾ÑÐµÐ½Ñ Ð¶Ð°Ð»Ñ, Ð½Ð¾ Ð·Ð°Ð¿ÑÐ¾ÑÑ Ñ Ð²Ð°ÑÐµÐ³Ð¾ ÑÑÑÑÐ¾Ð¹ÑÑÐ²Ð° Ð¿Ð¾ÑÐ¾Ð¶Ð¸ Ð½Ð° Ð°Ð²ÑÐ¾Ð¼Ð°ÑÐ¸ÑÐµÑÐºÐ¸Ðµ.Â  Â <a href=\"https://yandex.ru/support/smart-captcha?unique_key=a672f7af-6d19924a-a4b6b1a0-9777cc28\" class=\"Link Link_view_default\">ÐÐ¾ÑÐµÐ¼Ñ ÑÑÐ¾ Ð¼Ð¾Ð³Ð»Ð¾ Ð¿ÑÐ¾Ð¸Ð·Ð¾Ð¹ÑÐ¸?</a></span></div><noscript><span class=\"Text Text_color_alert Text_weight_medium Text_typography_body-long-m\">Ð£ Ð²Ð°Ñ Ð¾ÑÐºÐ»ÑÑÐµÐ½Ð¾ Ð¸ÑÐ¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ JavaScript. ÐÐ¾ Ð½Ð°Ð¶Ð°ÑÐ¸Ñ Ð²Ñ Ð±ÑÐ´ÐµÑÐµ Ð½Ð°Ð¿ÑÐ°Ð²Ð»ÐµÐ½Ñ Ð½Ð° Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸ÑÐµÐ»ÑÐ½ÑÑ Ð¿ÑÐ¾Ð²ÐµÑÐºÑ.Â <a href=\"https://yandex.ru/support/common/browsers-settings/browsers-java-js-settings.html\" class=\"Link Link_view_default\">ÐÐ°Ðº Ð²ÐºÐ»ÑÑÐ¸ÑÑ JavaScript?</a></span></noscript><div class=\"Spacer Spacer_auto-gap_bottom\" style=\"padding-top:40px;padding-bottom:40px\"><div class=\"CheckboxCaptcha\" data-testid=\"checkbox-captcha\"><div class=\"CheckboxCaptcha-Inner\"><div class=\"CheckboxCaptcha-Anchor\"><input type=\"button\" id=\"js-button\" class=\"CheckboxCaptcha-Button\" aria-checked=\"false\" aria-labelledby=\"smartcaptcha-status\" role=\"checkbox\"><noscript><input type=\"submit\" class=\"CheckboxCaptcha-Button\" aria-checked=\"false\" aria-labelledby=\"smartcaptcha-status\" role=\"checkbox\"></noscript><div class=\"CheckboxCaptcha-Checkbox\" data-checked=\"false\"><svg class=\"SvgIcon\" width=\"24\" height=\"24\" viewBox=\"0 0 24 25\" fill=\"none\"><path d=\"M4 12.5L9.5 18.5L20 6.5\" stroke=\"#000\" stroke-width=\"3\" stroke-linecap=\"round\" stroke-linejoin=\"round\"></path></svg></div></div><div class=\"CheckboxCaptcha-Label\"><span class=\"Text Text_weight_regular Text_typography_control-xxl CheckboxCaptcha-LabelText\">Ð¯ Ð½Ðµ ÑÐ¾Ð±Ð¾Ñ</span><span class=\"Text Text_color_control-secondary Text_weight_regular Text_typography_control-l CheckboxCaptcha-SecondaryText\">ÐÐ°Ð¶Ð¼Ð¸ÑÐµ, ÑÑÐ¾Ð±Ñ Ð¿ÑÐ¾Ð´Ð¾Ð»Ð¶Ð¸ÑÑ</span><span class=\"Text Text_color_ghost Text_weight_regular Text_typography_control-s CheckboxCaptcha-Links\"><a target=\"_blank\" href=\"https://cloud.yandex.ru/services/smartcaptcha\" aria-describedby=\"service-link-description\" class=\"Link Link_view_captcha\">Yandex SmartCaptcha</a><span class=\"Links-Dot\">â¢</span><a target=\"_blank\" href=\"https://yandex.ru/legal/smartcaptcha_notice/\" aria-describedby=\"connect-link-description\" class=\"Link Link_view_captcha\">ÐÐ±ÑÐ°Ð±Ð¾ÑÐºÐ° Ð´Ð°Ð½Ð½ÑÑ</a><div class=\"visuallyhidden\" id=\"service-link-description\">connect-link-description</div><div class=\"visuallyhidden\" id=\"privacy-link-description\">Ð¡Ð°Ð¹Ñ Ñ Ð¸Ð½ÑÐ¾ÑÐ¼Ð°ÑÐ¸ÐµÐ¹ Ð¾Ð± Ð¾Ð±ÑÐ°Ð±Ð¾ÑÐºÐµ Ð´Ð°Ð½Ð½ÑÑ</div></span></div><div class=\"CheckboxCaptcha-Icon\"><svg width=\"41\" height=\"20\" viewBox=\"0 0 41 20\" fill=\"none\" class=\"CloudIcon\"><path d=\"M10.251 11.0479C10.7785 12.2034 10.9542 12.6053 10.9542 13.9932V15.8333H9.07031V12.7309L5.51562 5H7.48142L10.251 11.0479ZM12.5746 5L10.2698 10.2376H12.1852L14.4963 5H12.5746Z\" fill=\"#000\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M10 18.2292C14.5448 18.2292 18.2292 14.5448 18.2292 10C18.2292 5.45516 14.5448 1.77083 10 1.77083C5.45516 1.77083 1.77083 5.45516 1.77083 10C1.77083 14.5448 5.45516 18.2292 10 18.2292ZM10 20C15.5228 20 20 15.5228 20 10C20 4.47715 15.5228 0 10 0C4.47715 0 0 4.47715 0 10C0 15.5228 4.47715 20 10 20Z\" fill=\"#000\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M39.2333 10C39.2333 14.55 35.55 18.2333 31 18.2333C29.8094 18.2333 28.9202 17.7864 28.4755 17.2443C28.0308 16.7022 27.9057 15.8152 28.0588 14.5339L30.7088 14.0082C33.13 13.5647 34.4641 12.2176 34.9417 9.80288C35.0001 9.46485 35.0669 9.12682 35.1301 8.8064C35.156 8.67525 35.1813 8.54705 35.2052 8.42301C35.2774 8.03104 35.3471 7.65855 35.411 7.31691C35.4783 6.95699 35.5392 6.63132 35.5899 6.3532C35.861 4.78329 35.6407 3.62423 35.0309 2.81788C37.5398 4.22801 39.2333 6.91477 39.2333 10ZM31 1.76667C26.45 1.76667 22.7667 5.45 22.7667 10C22.7667 13.0852 24.4602 15.772 26.9691 17.1821C26.3593 16.3758 26.139 15.2167 26.4101 13.6468C26.4684 13.3273 26.538 12.9507 26.6158 12.5305C26.67 12.2378 26.7281 11.9239 26.7889 11.5934C26.8128 11.4694 26.8381 11.3412 26.864 11.21C26.9273 10.8896 26.994 10.5516 27.0524 10.2136C27.5136 7.78235 28.8477 6.43534 31.2688 5.99181L33.9412 5.46614C34.0943 4.18484 33.9692 3.29778 33.5245 2.75569C33.0798 2.2136 32.1906 1.76667 31 1.76667ZM41 10C41 15.5167 36.5167 20 31 20C25.4833 20 21 15.5167 21 10C21 4.48333 25.4833 0 31 0C36.5167 0 41 4.48333 41 10ZM33.6241 7.30597L31.6147 7.70022C29.9018 8.02876 29.1112 8.81726 28.7818 10.5257C28.7052 10.9076 28.6385 11.2698 28.5741 11.6199C28.5554 11.7215 28.5368 11.8222 28.5183 11.922C28.4689 12.1848 28.4195 12.4476 28.3701 12.694L30.363 12.2998C32.0759 11.9712 32.883 11.1827 33.2124 9.47434C33.2848 9.11359 33.3515 8.75192 33.4182 8.39024C33.485 8.02852 33.5517 7.6668 33.6241 7.30597Z\" fill=\"#000\"></path></svg></div><div class=\"CheckboxCaptcha-Alert\"></div><div id=\"smartcaptcha-status\" class=\"CheckboxCaptcha-Status visuallyhidden\">SmartCaptcha Ð½ÑÐ¶Ð½Ð° Ð¿ÑÐ¾Ð²ÐµÑÐºÐ° Ð¿Ð¾Ð»ÑÐ·Ð¾Ð²Ð°ÑÐµÐ»Ñ</div></div></div></div><span class=\"Text Text_color_ghost Text_weight_regular Text_typography_control-xs\">ÐÑÐ»Ð¸ Ñ Ð²Ð°Ñ Ð²Ð¾Ð·Ð½Ð¸ÐºÐ»Ð¸ Ð¿ÑÐ¾Ð±Ð»ÐµÐ¼Ñ, Ð¿Ð¾Ð¶Ð°Ð»ÑÐ¹ÑÑÐ°, Ð²Ð¾ÑÐ¿Ð¾Ð»ÑÐ·ÑÐ¹ÑÐµÑÑÂ <a href=\"https://yandex.ru/support/smart-captcha?unique_key=a672f7af-6d19924a-a4b6b1a0-9777cc28#help\" class=\"Link Link_view_default\">ÑÐ¾ÑÐ¼Ð¾Ð¹ Ð¾Ð±ÑÐ°ÑÐ½Ð¾Ð¹ ÑÐ²ÑÐ·Ð¸</a></span><input type=\"hidden\" name=\"rdata\"><input type=\"hidden\" name=\"aesKey\"><input type=\"hidden\" name=\"signKey\"><input type=\"hidden\" name=\"pdata\"><input type=\"hidden\" name=\"tdata\"></form></div></div></div><script>const button=document.getElementById(\"js-button\");button.addEventListener(\"click\",function n(t){window.__JS_BUTTON_CLICKED__=!0,this.removeEventListener(\"click\",n,!1)},!1),window.onerror=function(n,t){0===t.indexOf(window.location.origin+\"/captcha_smart.a737a5c9dbdd19f5b829.min.js\")&&(button.type=\"submit\",window.Ya&&window.Ya.Rum&&window.Ya.Rum.logEventString&&window.Ya.Rum.logEventString(\"js_fail_force_submit_type\",n,{page:\"checkbox\"}))}</script> <script>window.__SSR_DATA__={url:\"/ru/checkbox\",invalid:\"no\",formAction:\"/checkcaptcha?key=e0b2cfe6-cbffca83-d1c8c-2ecc9be_2/1694881925/959657f24cad8037b858d37b4440b092_7a6fe709ed77d9c1ebe8bd8579b0e818&mt=2B60511570234A6F73DB7CE73B3817944EFDBBE5C7092515270ED26B239E1BFD52F698F441BD4F4602FE459AD81B1924BE1ADF086114AE6209AA8F99E41FA677F920D39B821DD7633F9C0A7BABF98400BAA0A66493921B286749C2414090CAA6163F7D90BF82703ACCEF6DFDE767&retpath=aHR0cHM6Ly95YS5ydS8__cb65b4c42c6b3840def67fee2954e15d&u=a672f7af-6d19924a-a4b6b1a0-9777cc28&s=b823da6d18a9c47d40e7bc2c6185de87\",captchaKey:\"e0b2cfe6-cbffca83-d1c8c-2ecc9be_2/1694881925/959657f24cad8037b858d37b4440b092_7a6fe709ed77d9c1ebe8bd8579b0e818\",imageSrc:\"\",taskImageSrc:\"\",voiceSrc:\"\",introSrc:\"\",aesKey:\"zmc9gKBUBaOSZ7r/xUXXQSq8my/GfOewX6L3MY9hNbg=\",aesSign:\"1_1694881925_2907227557850372060_ae1f7531693d592fe71700ec3ae47348\",reqId:\"1694881925588248-8784055439225661181-balancer-l7leveler-kubr-yp-vla-99-BAL\",uniqueKey:\"a672f7af-6d19924a-a4b6b1a0-9777cc28\",powPrefix:\"743D313639343838313932353B703D32613830623864652D34303762326131382D62666531393433392D36623735383737633B633D31353B643D39393037433643363746463633313141463237413230393335443039313633323B\",powComplexity:\"15\"}</script><script src=\"/captcha_smart_error.a737a5c9dbdd19f5b829.min.js?k=1688990308309\" crossorigin=\"\"></script><script src=\"https://yastatic.net/react/16.8.4/react-with-dom-and-polyfills.min.js\" crossorigin=\"\"></script><script src=\"/captcha_smart.a737a5c9dbdd19f5b829.min.js?k=1688990308309\" crossorigin=\"\"></script><script>!function(e,n,t,a,c){e.ym=e.ym||function(){(e.ym.a=e.ym.a||[]).push(arguments)},e.ym.l=+new Date,a=n.createElement(t),c=n.getElementsByTagName(t)[0],a.async=1,a.src=\"https://mc.yandex.ru/metrika/tag.js\",c.parentNode.insertBefore(a,c)}(window,document,\"script\"),ym(10630330,\"init\",{clickmap:!0,trackLinks:!0,accurateTrackBounce:!0,webvisor:!0,ut:\"noindex\",params:{req_id:\"1694881925588248-8784055439225661181-balancer-l7leveler-kubr-yp-vla-99-BAL\",unique_key:\"a672f7af-6d19924a-a4b6b1a0-9777cc28\"}})</script><noscript><div><img src=\"https://mc.yandex.ru/watch/10630330?ut=noindex\" style=\"position:absolute;left:-9999px\" alt=\"\"></div></noscript><div><img src=\"https://adfstat.yandex.ru/captcha?req_id=1694881925588248-8784055439225661181-balancer-l7leveler-kubr-yp-vla-99-BAL&unique_key=a672f7af-6d19924a-a4b6b1a0-9777cc28\" style=\"position:absolute;left:-9999px\" alt=\"\"></div>\n",
      "Thought: \n",
      "---\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens. However, your messages resulted in 6791 tokens. Please reduce the length of the messages.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mInvalidRequestError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[28], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43magent_executor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mwhat is on https://ya.ru/\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\langchain\\chains\\base.py:475\u001B[0m, in \u001B[0;36mChain.run\u001B[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001B[0m\n\u001B[0;32m    473\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    474\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`run` supports only one positional argument.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 475\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m)\u001B[49m[\n\u001B[0;32m    476\u001B[0m         _output_key\n\u001B[0;32m    477\u001B[0m     ]\n\u001B[0;32m    479\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[0;32m    480\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(kwargs, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, tags\u001B[38;5;241m=\u001B[39mtags, metadata\u001B[38;5;241m=\u001B[39mmetadata)[\n\u001B[0;32m    481\u001B[0m         _output_key\n\u001B[0;32m    482\u001B[0m     ]\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\langchain\\chains\\base.py:282\u001B[0m, in \u001B[0;36mChain.__call__\u001B[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001B[0m\n\u001B[0;32m    280\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    281\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[1;32m--> 282\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    283\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[0;32m    284\u001B[0m final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[0;32m    285\u001B[0m     inputs, outputs, return_only_outputs\n\u001B[0;32m    286\u001B[0m )\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\langchain\\chains\\base.py:276\u001B[0m, in \u001B[0;36mChain.__call__\u001B[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001B[0m\n\u001B[0;32m    270\u001B[0m run_manager \u001B[38;5;241m=\u001B[39m callback_manager\u001B[38;5;241m.\u001B[39mon_chain_start(\n\u001B[0;32m    271\u001B[0m     dumpd(\u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m    272\u001B[0m     inputs,\n\u001B[0;32m    273\u001B[0m )\n\u001B[0;32m    274\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    275\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m--> 276\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    277\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[0;32m    278\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[0;32m    279\u001B[0m     )\n\u001B[0;32m    280\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    281\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\langchain\\agents\\agent.py:1036\u001B[0m, in \u001B[0;36mAgentExecutor._call\u001B[1;34m(self, inputs, run_manager)\u001B[0m\n\u001B[0;32m   1034\u001B[0m \u001B[38;5;66;03m# We now enter the agent loop (until it returns something).\u001B[39;00m\n\u001B[0;32m   1035\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_continue(iterations, time_elapsed):\n\u001B[1;32m-> 1036\u001B[0m     next_step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_take_next_step\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1037\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname_to_tool_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1038\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolor_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1039\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1040\u001B[0m \u001B[43m        \u001B[49m\u001B[43mintermediate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1041\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1042\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1043\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(next_step_output, AgentFinish):\n\u001B[0;32m   1044\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_return(\n\u001B[0;32m   1045\u001B[0m             next_step_output, intermediate_steps, run_manager\u001B[38;5;241m=\u001B[39mrun_manager\n\u001B[0;32m   1046\u001B[0m         )\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\langchain\\agents\\agent.py:833\u001B[0m, in \u001B[0;36mAgentExecutor._take_next_step\u001B[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001B[0m\n\u001B[0;32m    830\u001B[0m     intermediate_steps \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_intermediate_steps(intermediate_steps)\n\u001B[0;32m    832\u001B[0m     \u001B[38;5;66;03m# Call the LLM to see what to do.\u001B[39;00m\n\u001B[1;32m--> 833\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magent\u001B[38;5;241m.\u001B[39mplan(\n\u001B[0;32m    834\u001B[0m         intermediate_steps,\n\u001B[0;32m    835\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39mrun_manager\u001B[38;5;241m.\u001B[39mget_child() \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    836\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs,\n\u001B[0;32m    837\u001B[0m     )\n\u001B[0;32m    838\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m OutputParserException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    839\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_parsing_errors, \u001B[38;5;28mbool\u001B[39m):\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\langchain\\agents\\agent.py:352\u001B[0m, in \u001B[0;36mLLMSingleActionAgent.plan\u001B[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    335\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mplan\u001B[39m(\n\u001B[0;32m    336\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    337\u001B[0m     intermediate_steps: List[Tuple[AgentAction, \u001B[38;5;28mstr\u001B[39m]],\n\u001B[0;32m    338\u001B[0m     callbacks: Callbacks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    339\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    340\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[AgentAction, AgentFinish]:\n\u001B[0;32m    341\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Given input, decided what to do.\u001B[39;00m\n\u001B[0;32m    342\u001B[0m \n\u001B[0;32m    343\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    350\u001B[0m \u001B[38;5;124;03m        Action specifying what tool to use.\u001B[39;00m\n\u001B[0;32m    351\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 352\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_chain\u001B[38;5;241m.\u001B[39mrun(\n\u001B[0;32m    353\u001B[0m         intermediate_steps\u001B[38;5;241m=\u001B[39mintermediate_steps,\n\u001B[0;32m    354\u001B[0m         stop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop,\n\u001B[0;32m    355\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39mcallbacks,\n\u001B[0;32m    356\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    357\u001B[0m     )\n\u001B[0;32m    358\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_parser\u001B[38;5;241m.\u001B[39mparse(output)\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\langchain\\chains\\base.py:480\u001B[0m, in \u001B[0;36mChain.run\u001B[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001B[0m\n\u001B[0;32m    475\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(args[\u001B[38;5;241m0\u001B[39m], callbacks\u001B[38;5;241m=\u001B[39mcallbacks, tags\u001B[38;5;241m=\u001B[39mtags, metadata\u001B[38;5;241m=\u001B[39mmetadata)[\n\u001B[0;32m    476\u001B[0m         _output_key\n\u001B[0;32m    477\u001B[0m     ]\n\u001B[0;32m    479\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[1;32m--> 480\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m)\u001B[49m[\n\u001B[0;32m    481\u001B[0m         _output_key\n\u001B[0;32m    482\u001B[0m     ]\n\u001B[0;32m    484\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[0;32m    485\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    486\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    487\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m but none were provided.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    488\u001B[0m     )\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\langchain\\chains\\base.py:282\u001B[0m, in \u001B[0;36mChain.__call__\u001B[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001B[0m\n\u001B[0;32m    280\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    281\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[1;32m--> 282\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    283\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[0;32m    284\u001B[0m final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[0;32m    285\u001B[0m     inputs, outputs, return_only_outputs\n\u001B[0;32m    286\u001B[0m )\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\langchain\\chains\\base.py:276\u001B[0m, in \u001B[0;36mChain.__call__\u001B[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001B[0m\n\u001B[0;32m    270\u001B[0m run_manager \u001B[38;5;241m=\u001B[39m callback_manager\u001B[38;5;241m.\u001B[39mon_chain_start(\n\u001B[0;32m    271\u001B[0m     dumpd(\u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m    272\u001B[0m     inputs,\n\u001B[0;32m    273\u001B[0m )\n\u001B[0;32m    274\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    275\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m--> 276\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    277\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[0;32m    278\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[0;32m    279\u001B[0m     )\n\u001B[0;32m    280\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    281\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\langchain\\chains\\llm.py:92\u001B[0m, in \u001B[0;36mLLMChain._call\u001B[1;34m(self, inputs, run_manager)\u001B[0m\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call\u001B[39m(\n\u001B[0;32m     88\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     89\u001B[0m     inputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[0;32m     90\u001B[0m     run_manager: Optional[CallbackManagerForChainRun] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     91\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m---> 92\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     93\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_outputs(response)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\langchain\\chains\\llm.py:102\u001B[0m, in \u001B[0;36mLLMChain.generate\u001B[1;34m(self, input_list, run_manager)\u001B[0m\n\u001B[0;32m    100\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Generate LLM result from inputs.\"\"\"\u001B[39;00m\n\u001B[0;32m    101\u001B[0m prompts, stop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_prompts(input_list, run_manager\u001B[38;5;241m=\u001B[39mrun_manager)\n\u001B[1;32m--> 102\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n\u001B[0;32m    103\u001B[0m     prompts,\n\u001B[0;32m    104\u001B[0m     stop,\n\u001B[0;32m    105\u001B[0m     callbacks\u001B[38;5;241m=\u001B[39mrun_manager\u001B[38;5;241m.\u001B[39mget_child() \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    106\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_kwargs,\n\u001B[0;32m    107\u001B[0m )\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\langchain\\chat_models\\base.py:415\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[1;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    407\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[0;32m    408\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    409\u001B[0m     prompts: List[PromptValue],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    412\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    413\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[0;32m    414\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[1;32m--> 415\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(prompt_messages, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\langchain\\chat_models\\base.py:310\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001B[0m\n\u001B[0;32m    308\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[0;32m    309\u001B[0m             run_managers[i]\u001B[38;5;241m.\u001B[39mon_llm_error(e)\n\u001B[1;32m--> 310\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    311\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    312\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)\n\u001B[0;32m    313\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[0;32m    314\u001B[0m ]\n\u001B[0;32m    315\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\langchain\\chat_models\\base.py:300\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001B[0m\n\u001B[0;32m    297\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[0;32m    298\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    299\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m--> 300\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_with_cache(\n\u001B[0;32m    301\u001B[0m                 m,\n\u001B[0;32m    302\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    303\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[i] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    304\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    305\u001B[0m             )\n\u001B[0;32m    306\u001B[0m         )\n\u001B[0;32m    307\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    308\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\langchain\\chat_models\\base.py:447\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    443\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    444\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    445\u001B[0m     )\n\u001B[0;32m    446\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported:\n\u001B[1;32m--> 447\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[0;32m    448\u001B[0m         messages, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    449\u001B[0m     )\n\u001B[0;32m    450\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    451\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\langchain\\chat_models\\openai.py:340\u001B[0m, in \u001B[0;36mChatOpenAI._generate\u001B[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001B[0m\n\u001B[0;32m    338\u001B[0m message_dicts, params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_message_dicts(messages, stop)\n\u001B[0;32m    339\u001B[0m params \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs}\n\u001B[1;32m--> 340\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompletion_with_retry(\n\u001B[0;32m    341\u001B[0m     messages\u001B[38;5;241m=\u001B[39mmessage_dicts, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\n\u001B[0;32m    342\u001B[0m )\n\u001B[0;32m    343\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_chat_result(response)\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\langchain\\chat_models\\openai.py:279\u001B[0m, in \u001B[0;36mChatOpenAI.completion_with_retry\u001B[1;34m(self, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;129m@retry_decorator\u001B[39m\n\u001B[0;32m    276\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_completion_with_retry\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m    277\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 279\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _completion_with_retry(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\tenacity\\__init__.py:289\u001B[0m, in \u001B[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001B[1;34m(*args, **kw)\u001B[0m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(f)\n\u001B[0;32m    288\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped_f\u001B[39m(\u001B[38;5;241m*\u001B[39margs: t\u001B[38;5;241m.\u001B[39mAny, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: t\u001B[38;5;241m.\u001B[39mAny) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m t\u001B[38;5;241m.\u001B[39mAny:\n\u001B[1;32m--> 289\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(f, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\tenacity\\__init__.py:379\u001B[0m, in \u001B[0;36mRetrying.__call__\u001B[1;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m    377\u001B[0m retry_state \u001B[38;5;241m=\u001B[39m RetryCallState(retry_object\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, fn\u001B[38;5;241m=\u001B[39mfn, args\u001B[38;5;241m=\u001B[39margs, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[0;32m    378\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 379\u001B[0m     do \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretry_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    380\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[0;32m    381\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\tenacity\\__init__.py:314\u001B[0m, in \u001B[0;36mBaseRetrying.iter\u001B[1;34m(self, retry_state)\u001B[0m\n\u001B[0;32m    312\u001B[0m is_explicit_retry \u001B[38;5;241m=\u001B[39m fut\u001B[38;5;241m.\u001B[39mfailed \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(fut\u001B[38;5;241m.\u001B[39mexception(), TryAgain)\n\u001B[0;32m    313\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (is_explicit_retry \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretry(retry_state)):\n\u001B[1;32m--> 314\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfut\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    316\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mafter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mafter(retry_state)\n",
      "File \u001B[1;32mC:\\Program Files\\python39_x64\\lib\\concurrent\\futures\\_base.py:439\u001B[0m, in \u001B[0;36mFuture.result\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    437\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[0;32m    438\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[1;32m--> 439\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    441\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_condition\u001B[38;5;241m.\u001B[39mwait(timeout)\n\u001B[0;32m    443\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001B[1;32mC:\\Program Files\\python39_x64\\lib\\concurrent\\futures\\_base.py:391\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    389\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[0;32m    390\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 391\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[0;32m    392\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    393\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[0;32m    394\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\tenacity\\__init__.py:382\u001B[0m, in \u001B[0;36mRetrying.__call__\u001B[1;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m    380\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[0;32m    381\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 382\u001B[0m         result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    383\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:  \u001B[38;5;66;03m# noqa: B902\u001B[39;00m\n\u001B[0;32m    384\u001B[0m         retry_state\u001B[38;5;241m.\u001B[39mset_exception(sys\u001B[38;5;241m.\u001B[39mexc_info())  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\langchain\\chat_models\\openai.py:277\u001B[0m, in \u001B[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001B[1;34m(**kwargs)\u001B[0m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;129m@retry_decorator\u001B[39m\n\u001B[0;32m    276\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_completion_with_retry\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m--> 277\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001B[0m, in \u001B[0;36mChatCompletion.create\u001B[1;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 25\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m TryAgain \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     27\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m>\u001B[39m start \u001B[38;5;241m+\u001B[39m timeout:\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001B[0m, in \u001B[0;36mEngineAPIResource.create\u001B[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001B[0m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[0;32m    129\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    136\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[0;32m    137\u001B[0m ):\n\u001B[0;32m    138\u001B[0m     (\n\u001B[0;32m    139\u001B[0m         deployment_id,\n\u001B[0;32m    140\u001B[0m         engine,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    150\u001B[0m         api_key, api_base, api_type, api_version, organization, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\n\u001B[0;32m    151\u001B[0m     )\n\u001B[1;32m--> 153\u001B[0m     response, _, api_key \u001B[38;5;241m=\u001B[39m \u001B[43mrequestor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpost\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    155\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    156\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    157\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    158\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    161\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    163\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[0;32m    164\u001B[0m         \u001B[38;5;66;03m# must be an iterator\u001B[39;00m\n\u001B[0;32m    165\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, OpenAIResponse)\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\openai\\api_requestor.py:298\u001B[0m, in \u001B[0;36mAPIRequestor.request\u001B[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001B[0m\n\u001B[0;32m    277\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[0;32m    278\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    279\u001B[0m     method,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    286\u001B[0m     request_timeout: Optional[Union[\u001B[38;5;28mfloat\u001B[39m, Tuple[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mfloat\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    287\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m    288\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest_raw(\n\u001B[0;32m    289\u001B[0m         method\u001B[38;5;241m.\u001B[39mlower(),\n\u001B[0;32m    290\u001B[0m         url,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    296\u001B[0m         request_timeout\u001B[38;5;241m=\u001B[39mrequest_timeout,\n\u001B[0;32m    297\u001B[0m     )\n\u001B[1;32m--> 298\u001B[0m     resp, got_stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    299\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m resp, got_stream, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_key\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\openai\\api_requestor.py:700\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response\u001B[1;34m(self, result, stream)\u001B[0m\n\u001B[0;32m    692\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m    693\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interpret_response_line(\n\u001B[0;32m    694\u001B[0m             line, result\u001B[38;5;241m.\u001B[39mstatus_code, result\u001B[38;5;241m.\u001B[39mheaders, stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    695\u001B[0m         )\n\u001B[0;32m    696\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m parse_stream(result\u001B[38;5;241m.\u001B[39miter_lines())\n\u001B[0;32m    697\u001B[0m     ), \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    698\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    699\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m--> 700\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response_line\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    701\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    702\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstatus_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    703\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    704\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    705\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m    706\u001B[0m         \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    707\u001B[0m     )\n",
      "File \u001B[1;32mF:\\openai-cookbook\\venv\\lib\\site-packages\\openai\\api_requestor.py:763\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response_line\u001B[1;34m(self, rbody, rcode, rheaders, stream)\u001B[0m\n\u001B[0;32m    761\u001B[0m stream_error \u001B[38;5;241m=\u001B[39m stream \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m resp\u001B[38;5;241m.\u001B[39mdata\n\u001B[0;32m    762\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream_error \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;241m200\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m rcode \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m300\u001B[39m:\n\u001B[1;32m--> 763\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_error_response(\n\u001B[0;32m    764\u001B[0m         rbody, rcode, resp\u001B[38;5;241m.\u001B[39mdata, rheaders, stream_error\u001B[38;5;241m=\u001B[39mstream_error\n\u001B[0;32m    765\u001B[0m     )\n\u001B[0;32m    766\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "\u001B[1;31mInvalidRequestError\u001B[0m: This model's maximum context length is 4097 tokens. However, your messages resulted in 6791 tokens. Please reduce the length of the messages."
     ]
    }
   ],
   "source": [
    "agent_executor.run('what is on https://ya.ru/')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T16:32:07.299373400Z",
     "start_time": "2023-09-16T16:32:04.453834400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff96040",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.run(\"How many in 2022?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b51817",
   "metadata": {},
   "source": [
    "## LLM Agent with History\n",
    "\n",
    "Extend the LLM Agent with the ability to retain a [memory](https://python.langchain.com/en/latest/modules/agents/agents/custom_llm_agent.html#adding-memory) and use it as context as it continues the conversation.\n",
    "\n",
    "We use a simple ```ConversationBufferWindowMemory``` for this example that keeps a rolling window of the last two conversation turns. LangChain has other [memory options](https://python.langchain.com/en/latest/modules/memory.html), with different tradeoffs suitable for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85737eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a prompt template which can interpolate the history\n",
    "template_with_history = \"\"\"You are SearchGPT, a professional search engine who provides informative answers to users. Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin! Remember to give detailed, informative answers\n",
    "\n",
    "Previous conversation history:\n",
    "{history}\n",
    "\n",
    "New question: {input}\n",
    "{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d85c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_with_history = CustomPromptTemplate(\n",
    "    template=template_with_history,\n",
    "    tools=tools,\n",
    "    # The history template includes \"history\" as an input variable so we can interpolate it into the prompt\n",
    "    input_variables=[\"input\", \"intermediate_steps\", \"history\"]\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_with_history)\n",
    "tool_names = [tool.name for tool in tools]\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"],\n",
    "    allowed_tools=tool_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c8fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the memory with k=2 to keep the last two turns\n",
    "# Provide the memory to the agent\n",
    "memory = ConversationBufferWindowMemory(k=2)\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f031bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.run(\"How many people live in canada as of 2023?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d28d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.run(\"how about in mexico?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62356eed",
   "metadata": {},
   "source": [
    "## Knowledge base\n",
    "\n",
    "Create a custom vectorstore for the Agent to use as a tool to answer questions with. We'll store the results in [Pinecone](https://docs.pinecone.io/docs/quickstart), which is supported by LangChain ([Docs](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/pinecone.html), [API reference](https://python.langchain.com/en/latest/reference/modules/vectorstore.html)). For help getting started with Pinecone or other vector databases, we have a [cookbook](https://github.com/openai/openai-cookbook/blob/colin/examples/vector_databases/Using_vector_databases_for_embeddings_search.ipynb) to help you get started.\n",
    "\n",
    "You can check the LangChain documentation to see what other [vectorstores](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html) and [databases](https://python.langchain.com/en/latest/modules/chains/examples/sqlite.html) are available.\n",
    "\n",
    "For this example we'll use the transcripts of the Stuff You Should Know podcast, which was provided thanks to OSF DOI [10.17605/OSF.IO/VM9NT](https://doi.org/10.17605/OSF.IO/VM9NT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f86008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "# Here is a URL to a zip archive containing the transcribed podcasts\n",
    "# Note that this data has already been split into chunks and embeddings from OpenAI's text-embedding-ada-002 embedding model are included\n",
    "content_url = 'https://cdn.openai.com/API/examples/data/sysk_podcast_transcripts_embedded.json.zip'\n",
    "\n",
    "# Download the file (it is ~541 MB so this will take some time)\n",
    "wget.download(content_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889c292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load podcasts\n",
    "with zipfile.ZipFile(\"sysk_podcast_transcripts_embedded.json.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./data\")\n",
    "f = open('./data/sysk_podcast_transcripts_embedded.json')\n",
    "processed_podcasts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070ef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the contents\n",
    "pd.DataFrame(processed_podcasts).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9715d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the text embeddings to Pinecone\n",
    "\n",
    "batch_size = 100  # how many embeddings we create and insert at once\n",
    "\n",
    "for i in tqdm(range(0, len(processed_podcasts), batch_size)):\n",
    "    # find end of batch\n",
    "    i_end = min(len(processed_podcasts), i + batch_size)\n",
    "    meta_batch = processed_podcasts[i:i_end]\n",
    "    # get ids\n",
    "    ids_batch = [x['cleaned_id'] for x in meta_batch]\n",
    "    # get texts to encode\n",
    "    texts = [x['text_chunk'] for x in meta_batch]\n",
    "    # add embeddings\n",
    "    embeds = [x['embedding'] for x in meta_batch]\n",
    "    # cleanup metadata\n",
    "    meta_batch = [{\n",
    "        'filename':   x['filename'],\n",
    "        'title':      x['title'],\n",
    "        'text_chunk': x['text_chunk'],\n",
    "        'url':        x['url']\n",
    "    } for x in meta_batch]\n",
    "    to_upsert = list(zip(ids_batch, embeds, meta_batch))\n",
    "    # upsert to Pinecone\n",
    "    index.upsert(vectors=to_upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5bac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the embeddings to be used by our retriever to be OpenAI Embeddings, matching our embedded corpus\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Loads a docsearch object from an existing Pinecone index so we can retrieve from it\n",
    "docsearch = Pinecone.from_existing_index(index_name, embeddings, text_key='text_chunk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd21d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abd34e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_docs = retriever.get_relevant_documents(\"can you live without a bank account\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016bd458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the title and content for the most relevant retrieved documents\n",
    "print(\"\\n\".join(['Title: ' + x.metadata['title'].strip() + '\\n\\n' + x.page_content + '\\n\\n' for x in query_docs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf10f53",
   "metadata": {},
   "source": [
    "## LLM Agent with Tools\n",
    "\n",
    "Extend our list of tools by creating a [RetrievalQA](https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html) chain leveraging our Pinecone knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cae4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retrieval_llm = OpenAI(temperature=0)\n",
    "\n",
    "podcast_retriever = RetrievalQA.from_chain_type(llm=retrieval_llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc282cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name='Knowledge Base',\n",
    "        func=podcast_retriever.run,\n",
    "        description=\"Useful for general questions about how to do things and for details on interesting topics. Input should be a fully formed question.\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e12121",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Re-initialize the agent with our new list of tools\n",
    "prompt_with_history = CustomPromptTemplate(\n",
    "    template=template_with_history,\n",
    "    tools=expanded_tools,\n",
    "    input_variables=[\"input\", \"intermediate_steps\", \"history\"]\n",
    ")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_with_history)\n",
    "multi_tool_names = [tool.name for tool in expanded_tools]\n",
    "multi_tool_agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"],\n",
    "    allowed_tools=multi_tool_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_tool_memory = ConversationBufferWindowMemory(k=2)\n",
    "multi_tool_executor = AgentExecutor.from_agent_and_tools(agent=multi_tool_agent, tools=expanded_tools, verbose=True, memory=multi_tool_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27bb1de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "multi_tool_executor.run(\"Hi, I'd like to know how you can live without a bank account\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba815cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_tool_executor.run('Can you tell me some interesting facts about whether zoos are good or bad for animals')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe408a3",
   "metadata": {},
   "source": [
    "You now have a template to deploy conversational agents with tools. If you want to extend this with a Custom Agent to add your own retry behaviour or treatment of input/output variables, then follow [this article](https://python.langchain.com/en/latest/modules/agents/agents/custom_agent.html).\n",
    "\n",
    "We look forward to seeing what you build!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384f72b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
