{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to format inputs to ChatGPT models\n",
    "\n",
    "ChatGPT is powered by `gpt-3.5-turbo` and `gpt-4`, OpenAI's most advanced models.\n",
    "\n",
    "You can build your own applications with `gpt-3.5-turbo` or `gpt-4` using the OpenAI API.\n",
    "\n",
    "Chat models take a series of messages as input, and return an AI-written message as output.\n",
    "\n",
    "This guide illustrates the chat format with a few example API calls."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the openai library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, install and/or upgrade to the latest version of the OpenAI Python library\n",
    "%pip install --upgrade openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T03:03:07.275892100Z",
     "start_time": "2023-08-15T03:03:06.864257600Z"
    }
   },
   "outputs": [],
   "source": [
    "# import the OpenAI Python library for calling the OpenAI API\n",
    "import openai\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. An example chat API call\n",
    "\n",
    "A chat API call has two required inputs:\n",
    "- `model`: the name of the model you want to use (e.g., `gpt-3.5-turbo`, `gpt-4`, `gpt-3.5-turbo-0613`, `gpt-3.5-turbo-16k-0613`)\n",
    "- `messages`: a list of message objects, where each object has two required fields:\n",
    "    - `role`: the role of the messenger (either `system`, `user`, or `assistant`)\n",
    "    - `content`: the content of the message (e.g., `Write me a beautiful poem`)\n",
    "\n",
    "Messages can also contain an optional `name` field, which give the messenger a name. E.g., `example-user`, `Alice`, `BlackbeardBot`. Names may not contain spaces.\n",
    "\n",
    "As of June 2023, you can also optionally submit a list of `functions` that tell GPT whether it can generate JSON to feed into a function. For details, see the [documentation](https://platform.openai.com/docs/guides/gpt/function-calling), [API reference](https://platform.openai.com/docs/api-reference/chat), or the Cookbook guide [How to call functions with chat models](How_to_call_functions_with_chat_models.ipynb).\n",
    "\n",
    "Typically, a conversation will start with a system message that tells the assistant how to behave, followed by alternating user and assistant messages, but you are not required to follow this format.\n",
    "\n",
    "Let's look at an example chat API calls to see how the chat format works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example OpenAI Python library request\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Orange.\"},\n",
    "    ],\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "response\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the response object has a few fields:\n",
    "- `id`: the ID of the request\n",
    "- `object`: the type of object returned (e.g., `chat.completion`)\n",
    "- `created`: the timestamp of the request\n",
    "- `model`: the full name of the model used to generate the response\n",
    "- `usage`: the number of tokens used to generate the replies, counting prompt, completion, and total\n",
    "- `choices`: a list of completion objects (only one, unless you set `n` greater than 1)\n",
    "    - `message`: the message object generated by the model, with `role` and `content`\n",
    "    - `finish_reason`: the reason the model stopped generating text (either `stop`, or `length` if `max_tokens` limit was reached)\n",
    "    - `index`: the index of the completion in the list of choices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract just the reply with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response['choices'][0]['message']['content']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even non-conversation-based tasks can fit into the chat format, by placing the instruction in the first user message.\n",
    "\n",
    "For example, to ask the model to explain asynchronous programming in the style of the pirate Blackbeard, we can structure conversation as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example with a system message\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain protocol enumerating in the style of D. Trump\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example without a system message\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain protocol enumerating in the style of D. Trump\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tips for instructing gpt-3.5-turbo-0301\n",
    "\n",
    "Best practices for instructing models may change from model version to model version. The advice that follows applies to `gpt-3.5-turbo-0301` and may not apply to future models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System messages\n",
    "\n",
    "The system message can be used to prime the assistant with different personalities or behaviors.\n",
    "\n",
    "Be aware that `gpt-3.5-turbo-0301` does not generally pay as much attention to the system message as `gpt-4-0314` or `gpt-3.5-turbo-0613`. Therefore, for `gpt-3.5-turbo-0301`, we recommend placing important instructions in the user message instead. Some developers have found success in continually moving the system message near the end of the conversation to keep the model's attention from drifting away as conversations get longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of a system message that primes the assistant to explain concepts in great depth\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a friendly and helpful teaching assistant. You explain concepts in great depth using simple terms, and you give examples to help people learn. At the end of each explanation, you ask a question to check for understanding\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you explain how fractions work?\"},\n",
    "    ],\n",
    "    temperature=0.5,\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of a system message that primes the assistant to give brief, to-the-point answers\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a laconic assistant. You reply with brief, to-the-point answers with no elaboration but \"\n",
    "                                      \"with an 3 examples of assisted thing\"\n",
    "                                      \".\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you explain how fractions work?\"},\n",
    "    ],\n",
    "    temperature=0.9,\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Few-shot prompting\n",
    "\n",
    "In some cases, it's easier to show the model what you want rather than tell the model what you want.\n",
    "\n",
    "One way to show the model what you want is with faked example messages.\n",
    "\n",
    "For example:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},\n",
    "        {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
    "        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help clarify that the example messages are not part of a real conversation, and shouldn't be referred back to by the model, you can try setting the `name` field of `system` messages to `example_user` and `example_assistant`.\n",
    "\n",
    "Transforming the few-shot example above, we could write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The business jargon translation example, but with example names for the example messages\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain \"\n",
    "                                      \" hexed word strings in digits.\"},\n",
    "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
    "        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
    "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
    "        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
    "        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not every attempt at engineering conversations will succeed at first.\n",
    "\n",
    "If your first attempts fail, don't be afraid to experiment with different ways of priming or conditioning the model.\n",
    "\n",
    "As an example, one developer discovered an increase in accuracy when they inserted a user message that said \"Great job so far, these have been perfect\" to help condition the model into providing higher quality responses.\n",
    "\n",
    "For more ideas on how to lift the reliability of the models, consider reading our guide on [techniques to increase reliability](../techniques_to_improve_reliability.md). It was written for non-chat models, but many of its principles still apply."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Counting tokens\n",
    "\n",
    "When you submit your request, the API transforms the messages into a sequence of tokens.\n",
    "\n",
    "The number of tokens used affects:\n",
    "- the cost of the request\n",
    "- the time it takes to generate the response\n",
    "- when the reply gets cut off from hitting the maximum token limit (4,096 for `gpt-3.5-turbo` or 8,192 for `gpt-4`)\n",
    "\n",
    "You can use the following function to count the number of tokens that a list of messages will use.\n",
    "\n",
    "Note that the exact way that tokens are counted from messages may change from model to model. Consider the counts from the function below an estimate, not a timeless guarantee.\n",
    "\n",
    "In particular, requests that use the optional functions input will consume extra tokens on top of the estimates calculated below.\n",
    "\n",
    "Read more about counting tokens in [How to count tokens with tiktoken](How_to_count_tokens_with_tiktoken.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T02:30:31.194721Z",
     "start_time": "2023-08-15T02:30:31.033726200Z"
    }
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\u001B[32mfound 11\u001B[0m\n",
      "INFO:root:\u001B[31mTitle: I'm Hearing (Different) Voices: Anonymous Voices to Protect User Privacy\u001B[0m\n",
      "INFO:root:\u001B[97mDate: 2022-02-13\u001B[0m\n",
      "INFO:root:Summary: In this paper, we present AltVoice -- a system designed to help user's\n",
      "protect their privacy when using remotely accessed voice services. The system\n",
      "allows a user to conceal their true voice identity information with no\n",
      "cooperation from the remote voice service: AltVoice re-synthesizes user's\n",
      "spoken audio to sound as if it has been spoken by a different, private\n",
      "identity. The system converts audio to its textual representation at its\n",
      "midpoint, and thus removes any linkage between the user's voice and the\n",
      "generated private voices. We implement AltVoice and we propose six different\n",
      "methods to generate private voice identities, each is based on a user-known\n",
      "secret. We identify the system's trade-offs, and we investigate them for each\n",
      "of the proposed identity generation methods. Specifically, we investigate\n",
      "generated voices' diversity, word error rate, perceived speech quality and the\n",
      "success of attackers under privacy compromise and authentication compromise\n",
      "attack scenarios. Our results show that AltVoice-generated voices are not\n",
      "easily linked to original users, enabling users to protect themselves from\n",
      "voice data leakages and allowing for the revocability of (generated) voice\n",
      "data; akin to using passwords. However the results also show further work is\n",
      "needed on ensuring that the produced audio is natural, and that identities of\n",
      "private voices are distinct from one another. We discuss the future steps into\n",
      "improving AltVoice and the new implications that its existence has for the\n",
      "creations of remotely accessed voice services.\u001B[0m\n",
      "INFO:root:------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "INFO:root:\u001B[31mTitle: Beyond Voice Identity Conversion: Manipulating Voice Attributes by\n",
      "  Adversarial Learning of Structured Disentangled Representations\u001B[0m\n",
      "INFO:root:\u001B[97mDate: 2021-07-26\u001B[0m\n",
      "INFO:root:Summary: Voice conversion (VC) consists of digitally altering the voice of an\n",
      "individual to manipulate part of its content, primarily its identity, while\n",
      "maintaining the rest unchanged. Research in neural VC has accomplished\n",
      "considerable breakthroughs with the capacity to falsify a voice identity using\n",
      "a small amount of data with a highly realistic rendering. This paper goes\n",
      "beyond voice identity and presents a neural architecture that allows the\n",
      "manipulation of voice attributes (e.g., gender and age). Leveraging the latest\n",
      "advances on adversarial learning of structured speech representation, a novel\n",
      "structured neural network is proposed in which multiple auto-encoders are used\n",
      "to encode speech as a set of idealistically independent linguistic and\n",
      "extra-linguistic representations, which are learned adversariarly and can be\n",
      "manipulated during VC. Moreover, the proposed architecture is time-synchronized\n",
      "so that the original voice timing is preserved during conversion which allows\n",
      "lip-sync applications. Applied to voice gender conversion on the real-world\n",
      "VCTK dataset, our proposed architecture can learn successfully\n",
      "gender-independent representation and convert the voice gender with a very high\n",
      "efficiency and naturalness.\u001B[0m\n",
      "INFO:root:------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "INFO:root:\u001B[31mTitle: Skull-stripping for Tumor-bearing Brain Images\u001B[0m\n",
      "INFO:root:\u001B[97mDate: 2012-04-02\u001B[0m\n",
      "INFO:root:Summary: Skull-stripping separates the skull region of the head from the soft brain\n",
      "tissues. In many cases of brain image analysis, this is an essential\n",
      "preprocessing step in order to improve the final result. This is true for both\n",
      "registration and segmentation tasks. In fact, skull-stripping of magnetic\n",
      "resonance images (MRI) is a well-studied problem with numerous publications in\n",
      "recent years. Many different algorithms have been proposed, a summary and\n",
      "comparison of which can be found in [Fennema-Notestine, 2006]. Despite the\n",
      "abundance of approaches, we discovered that the algorithms which had been\n",
      "suggested so far, perform poorly when dealing with tumor-bearing brain images.\n",
      "This is mostly due to additional difficulties in separating the brain from the\n",
      "skull in this case, especially when the lesion is located very close to the\n",
      "skull border. Additionally, images acquired according to standard clinical\n",
      "protocols, often exhibit anisotropic resolution and only partial coverage,\n",
      "which further complicates the task. Therefore, we developed a method which is\n",
      "dedicated to skull-stripping for clinically acquired tumor-bearing brain\n",
      "images.\u001B[0m\n",
      "INFO:root:------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "INFO:root:\u001B[31mTitle: Semantics-Consistent Representation Learning for Remote Sensing\n",
      "  Image-Voice Retrieval\u001B[0m\n",
      "INFO:root:\u001B[97mDate: 2021-03-09\u001B[0m\n",
      "INFO:root:Summary: With the development of earth observation technology, massive amounts of\n",
      "remote sensing (RS) images are acquired. To find useful information from these\n",
      "images, cross-modal RS image-voice retrieval provides a new insight. This paper\n",
      "aims to study the task of RS image-voice retrieval so as to search effective\n",
      "information from massive amounts of RS data. Existing methods for RS\n",
      "image-voice retrieval rely primarily on the pairwise relationship to narrow the\n",
      "heterogeneous semantic gap between images and voices. However, apart from the\n",
      "pairwise relationship included in the datasets, the intra-modality and\n",
      "non-paired inter-modality relationships should also be taken into account\n",
      "simultaneously, since the semantic consistency among non-paired representations\n",
      "plays an important role in the RS image-voice retrieval task. Inspired by this,\n",
      "a semantics-consistent representation learning (SCRL) method is proposed for RS\n",
      "image-voice retrieval. The main novelty is that the proposed method takes the\n",
      "pairwise, intra-modality, and non-paired inter-modality relationships into\n",
      "account simultaneously, thereby improving the semantic consistency of the\n",
      "learned representations for the RS image-voice retrieval. The proposed SCRL\n",
      "method consists of two main steps: 1) semantics encoding and 2)\n",
      "semantics-consistent representation learning. Firstly, an image encoding\n",
      "network is adopted to extract high-level image features with a transfer\n",
      "learning strategy, and a voice encoding network with dilated convolution is\n",
      "devised to obtain high-level voice features. Secondly, a consistent\n",
      "representation space is conducted by modeling the three kinds of relationships\n",
      "to narrow the heterogeneous semantic gap and learn semantics-consistent\n",
      "representations across two modalities. Extensive experimental results on three\n",
      "challenging RS image-voice datasets show the effectiveness of the proposed\n",
      "method.\u001B[0m\n",
      "INFO:root:------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "INFO:root:\u001B[31mTitle: Make-A-Voice: Unified Voice Synthesis With Discrete Representation\u001B[0m\n",
      "INFO:root:\u001B[97mDate: 2023-05-30\u001B[0m\n",
      "INFO:root:Summary: Various applications of voice synthesis have been developed independently\n",
      "despite the fact that they generate \"voice\" as output in common. In addition,\n",
      "the majority of voice synthesis models currently rely on annotated audio data,\n",
      "but it is crucial to scale them to self-supervised datasets in order to\n",
      "effectively capture the wide range of acoustic variations present in human\n",
      "voice, including speaker identity, emotion, and prosody. In this work, we\n",
      "propose Make-A-Voice, a unified framework for synthesizing and manipulating\n",
      "voice signals from discrete representations. Make-A-Voice leverages a\n",
      "\"coarse-to-fine\" approach to model the human voice, which involves three\n",
      "stages: 1) semantic stage: model high-level transformation between linguistic\n",
      "content and self-supervised semantic tokens, 2) acoustic stage: introduce\n",
      "varying control signals as acoustic conditions for semantic-to-acoustic\n",
      "modeling, and 3) generation stage: synthesize high-fidelity waveforms from\n",
      "acoustic tokens. Make-A-Voice offers notable benefits as a unified voice\n",
      "synthesis framework: 1) Data scalability: the major backbone (i.e., acoustic\n",
      "and generation stage) does not require any annotations, and thus the training\n",
      "data could be scaled up. 2) Controllability and conditioning flexibility: we\n",
      "investigate different conditioning mechanisms and effectively handle three\n",
      "voice synthesis applications, including text-to-speech (TTS), voice conversion\n",
      "(VC), and singing voice synthesis (SVS) by re-synthesizing the discrete voice\n",
      "representations with prompt guidance. Experimental results demonstrate that\n",
      "Make-A-Voice exhibits superior audio quality and style similarity compared with\n",
      "competitive baseline models. Audio samples are available at\n",
      "https://Make-A-Voice.github.io\u001B[0m\n",
      "INFO:root:------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "INFO:root:\u001B[31mTitle: NANSY++: Unified Voice Synthesis with Neural Analysis and Synthesis\u001B[0m\n",
      "INFO:root:\u001B[97mDate: 2022-11-17\u001B[0m\n",
      "INFO:root:Summary: Various applications of voice synthesis have been developed independently\n",
      "despite the fact that they generate \"voice\" as output in common. In addition,\n",
      "most of the voice synthesis models still require a large number of audio data\n",
      "paired with annotated labels (e.g., text transcription and music score) for\n",
      "training. To this end, we propose a unified framework of synthesizing and\n",
      "manipulating voice signals from analysis features, dubbed NANSY++. The backbone\n",
      "network of NANSY++ is trained in a self-supervised manner that does not require\n",
      "any annotations paired with audio. After training the backbone network, we\n",
      "efficiently tackle four voice applications - i.e. voice conversion,\n",
      "text-to-speech, singing voice synthesis, and voice designing - by partially\n",
      "modeling the analysis features required for each task. Extensive experiments\n",
      "show that the proposed framework offers competitive advantages such as\n",
      "controllability, data efficiency, and fast training convergence, while\n",
      "providing high quality synthesis. Audio samples: tinyurl.com/8tnsy3uc.\u001B[0m\n",
      "INFO:root:------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "INFO:root:\u001B[31mTitle: Identifying Source Speakers for Voice Conversion based Spoofing Attacks\n",
      "  on Speaker Verification Systems\u001B[0m\n",
      "INFO:root:\u001B[97mDate: 2022-06-18\u001B[0m\n",
      "INFO:root:Summary: An automatic speaker verification system aims to verify the speaker identity\n",
      "of a speech signal. However, a voice conversion system could manipulate a\n",
      "person's speech signal to make it sound like another speaker's voice and\n",
      "deceive the speaker verification system. Most countermeasures for voice\n",
      "conversion-based spoofing attacks are designed to discriminate bona fide speech\n",
      "from spoofed speech for speaker verification systems. In this paper, we\n",
      "investigate the problem of source speaker identification -- inferring the\n",
      "identity of the source speaker given the voice converted speech. To perform\n",
      "source speaker identification, we simply add voice-converted speech data with\n",
      "the label of source speaker identity to the genuine speech dataset during\n",
      "speaker embedding network training. Experimental results show the feasibility\n",
      "of source speaker identification when training and testing with converted\n",
      "speeches from the same voice conversion model(s). In addition, our results\n",
      "demonstrate that having more converted utterances from various voice conversion\n",
      "model for training helps improve the source speaker identification performance\n",
      "on converted utterances from unseen voice conversion models.\u001B[0m\n",
      "INFO:root:------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "INFO:root:\u001B[31mTitle: A Voice Disease Detection Method Based on MFCCs and Shallow CNN\u001B[0m\n",
      "INFO:root:\u001B[97mDate: 2023-04-18\u001B[0m\n",
      "INFO:root:Summary: The incidence rate of voice diseases is increasing year by year. The use of\n",
      "software for remote diagnosis is a technical development trend and has\n",
      "important practical value. Among voice diseases, common diseases that cause\n",
      "hoarseness include spasmodic dysphonia, vocal cord paralysis, vocal nodule, and\n",
      "vocal cord polyp. This paper presents a voice disease detection method that can\n",
      "be applied in a wide range of clinical. We cooperated with Xiangya Hospital of\n",
      "Central South University to collect voice samples from sixty-one different\n",
      "patients. The Mel Frequency Cepstrum Coefficient (MFCC) parameters are\n",
      "extracted as input features to describe the voice in the form of data. An\n",
      "innovative model combining MFCC parameters and single convolution layer CNN is\n",
      "proposed for fast calculation and classification. The highest accuracy we\n",
      "achieved was 92%, it is fully ahead of the original research results and\n",
      "internationally advanced. And we use Advanced Voice Function Assessment\n",
      "Databases (AVFAD) to evaluate the generalization ability of the method we\n",
      "proposed, which achieved an accuracy rate of 98%. Experiments on clinical and\n",
      "standard datasets show that for the pathological detection of voice diseases,\n",
      "our method has greatly improved in accuracy and computational efficiency.\u001B[0m\n",
      "INFO:root:------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "INFO:root:\u001B[31mTitle: Secure voice based authentication for mobile devices: Vaulted Voice\n",
      "  Verification\u001B[0m\n",
      "INFO:root:\u001B[97mDate: 2012-11-30\u001B[0m\n",
      "INFO:root:Summary: As the use of biometrics becomes more wide-spread, the privacy concerns that\n",
      "stem from the use of biometrics are becoming more apparent. As the usage of\n",
      "mobile devices grows, so does the desire to implement biometric identification\n",
      "into such devices. A large majority of mobile devices being used are mobile\n",
      "phones. While work is being done to implement different types of biometrics\n",
      "into mobile phones, such as photo based biometrics, voice is a more natural\n",
      "choice. The idea of voice as a biometric identifier has been around a long\n",
      "time. One of the major concerns with using voice as an identifier is the\n",
      "instability of voice. We have developed a protocol that addresses those\n",
      "instabilities and preserves privacy. This paper describes a novel protocol that\n",
      "allows a user to authenticate using voice on a mobile/remote device without\n",
      "compromising their privacy. We first discuss the \\vv protocol, which has\n",
      "recently been introduced in research literature, and then describe its\n",
      "limitations. We then introduce a novel adaptation and extension of the vaulted\n",
      "verification protocol to voice, dubbed $V^3$. Following that we show a\n",
      "performance evaluation and then conclude with a discussion of security and\n",
      "future work.\u001B[0m\n",
      "INFO:root:------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "INFO:root:\u001B[31mTitle: PitchNet: Unsupervised Singing Voice Conversion with Pitch Adversarial\n",
      "  Network\u001B[0m\n",
      "INFO:root:\u001B[97mDate: 2019-12-04\u001B[0m\n",
      "INFO:root:Summary: Singing voice conversion is to convert a singer's voice to another one's\n",
      "voice without changing singing content. Recent work shows that unsupervised\n",
      "singing voice conversion can be achieved with an autoencoder-based approach\n",
      "[1]. However, the converted singing voice can be easily out of key, showing\n",
      "that the existing approach cannot model the pitch information precisely. In\n",
      "this paper, we propose to advance the existing unsupervised singing voice\n",
      "conversion method proposed in [1] to achieve more accurate pitch translation\n",
      "and flexible pitch manipulation. Specifically, the proposed PitchNet added an\n",
      "adversarially trained pitch regression network to enforce the encoder network\n",
      "to learn pitch invariant phoneme representation, and a separate module to feed\n",
      "pitch extracted from the source audio to the decoder network. Our evaluation\n",
      "shows that the proposed method can greatly improve the quality of the converted\n",
      "singing voice (2.92 vs 3.75 in MOS). We also demonstrate that the pitch of\n",
      "converted singing can be easily controlled during generation by changing the\n",
      "levels of the extracted pitch before passing it to the decoder network.\u001B[0m\n",
      "INFO:root:------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "INFO:root:\u001B[31mTitle: Towards Voice Reconstruction from EEG during Imagined Speech\u001B[0m\n",
      "INFO:root:\u001B[97mDate: 2023-01-02\u001B[0m\n",
      "INFO:root:Summary: Translating imagined speech from human brain activity into voice is a\n",
      "challenging and absorbing research issue that can provide new means of human\n",
      "communication via brain signals. Endeavors toward reconstructing speech from\n",
      "brain activity have shown their potential using invasive measures of spoken\n",
      "speech data, however, have faced challenges in reconstructing imagined speech.\n",
      "In this paper, we propose NeuroTalk, which converts non-invasive brain signals\n",
      "of imagined speech into the user's own voice. Our model was trained with spoken\n",
      "speech EEG which was generalized to adapt to the domain of imagined speech,\n",
      "thus allowing natural correspondence between the imagined speech and the voice\n",
      "as a ground truth. In our framework, automatic speech recognition decoder\n",
      "contributed to decomposing the phonemes of generated speech, thereby displaying\n",
      "the potential of voice reconstruction from unseen words. Our results imply the\n",
      "potential of speech synthesis from human EEG signals, not only from spoken\n",
      "speech but also from the brain signals of imagined speech.\u001B[0m\n",
      "INFO:root:------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import logging\n",
    "import xml.etree.ElementTree as ET\n",
    "from pprint import pprint\n",
    "import json\n",
    "from termcolor import colored as c\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Define the base URL and search query parameters\n",
    "base_url = 'http://export.arxiv.org/api/query?'\n",
    "query = \"voices in head, voices in skull, remote brain manipulation\"\n",
    "params = {'search_query': query, 'start': 0, 'max_results': 11}\n",
    "\n",
    "# Send a GET request to the ArXiv API\n",
    "response = requests.get(base_url, params=params)\n",
    "da = json.dumps(response.content.decode('utf-8'))\n",
    "\n",
    "# Parse the XML response and retrieve the necessary fields\n",
    "root = ET.fromstring(response.content)\n",
    "entries = root.findall('{http://www.w3.org/2005/Atom}entry')\n",
    "\n",
    "# Extract the title, date, and summary for each document and print them\n",
    "logging.info(colored(f'found {len(entries)}', 'green'))\n",
    "for entry in entries:\n",
    "    title = entry.find('{http://www.w3.org/2005/Atom}title').text.strip()\n",
    "    published_date = entry.find('{http://www.w3.org/2005/Atom}published').text.strip()[:10]\n",
    "    summary = entry.find('{http://www.w3.org/2005/Atom}summary').text.strip()\n",
    "\n",
    "    logging.info(c(f'Title: {title}', 'red'))\n",
    "    logging.info(c(f'Date: {published_date}', 'white'))\n",
    "    logging.info(c(f'Summary: {summary}'))\n",
    "    logging.info('---' * 50)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T02:36:53.573863200Z",
     "start_time": "2023-08-15T02:36:52.822864200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's verify the function above matches the OpenAI API response\n",
    "\n",
    "import openai\n",
    "\n",
    "example_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"New synergies will help drive top-line growth.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Things working well together will increase revenue.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for model in [\n",
    "    \"gpt-3.5-turbo-0301\",\n",
    "    \"gpt-3.5-turbo-0613\",\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"gpt-4-0314\",\n",
    "    \"gpt-4-0613\",\n",
    "    \"gpt-4\",\n",
    "    ]:\n",
    "    print(model)\n",
    "    # example token count from the function defined above\n",
    "    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n",
    "    # example token count from the OpenAI API\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=example_messages,\n",
    "        temperature=0,\n",
    "        max_tokens=1,  # we're only counting input tokens here, so let's not waste tokens on the output\n",
    "    )\n",
    "    print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens counted by the OpenAI API.')\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
